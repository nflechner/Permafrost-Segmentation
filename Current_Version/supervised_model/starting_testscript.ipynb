{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 239/239 [06:55<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Jaccard Score: 0.0729, Target Class Jaccard Score: 0.1458\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 239/239 [08:56<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Jaccard Score: 0.0729, Target Class Jaccard Score: 0.1458\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4:   1%|▏         | 3/239 [00:05<06:36,  1.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 185\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    184\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:  \n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# get the inputs;\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    188\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m, in \u001b[0;36mSemanticSegmentationDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     59\u001b[0m segmentation_map \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(ann_path)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# randomly crop + pad both image and segmentation map to same size\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegmentation_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m encoded_inputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     65\u001b[0m   encoded_inputs[k]\u001b[38;5;241m.\u001b[39msqueeze_() \u001b[38;5;66;03m# remove batch dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:303\u001b[0m, in \u001b[0;36mSegformerImageProcessor.__call__\u001b[0;34m(self, images, segmentation_maps, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, segmentation_maps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    Preprocesses a batch of images and optionally segmentation maps.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m    Overrides the `__call__` method of the `Preprocessor` class so that both images and segmentation maps can be\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m    passed in as positional arguments.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegmentation_maps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegmentation_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/utils/generic.py:852\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    845\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    849\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    850\u001b[0m     )\n\u001b[0;32m--> 852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:403\u001b[0m, in \u001b[0;36mSegformerImageProcessor.preprocess\u001b[0;34m(self, images, segmentation_maps, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_reduce_labels, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m     )\n\u001b[1;32m    392\u001b[0m validate_preprocess_arguments(\n\u001b[1;32m    393\u001b[0m     do_rescale\u001b[38;5;241m=\u001b[39mdo_rescale,\n\u001b[1;32m    394\u001b[0m     rescale_factor\u001b[38;5;241m=\u001b[39mrescale_factor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m    401\u001b[0m )\n\u001b[0;32m--> 403\u001b[0m images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_image(\n\u001b[1;32m    405\u001b[0m         image\u001b[38;5;241m=\u001b[39mimg,\n\u001b[1;32m    406\u001b[0m         do_resize\u001b[38;5;241m=\u001b[39mdo_resize,\n\u001b[1;32m    407\u001b[0m         resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m    408\u001b[0m         size\u001b[38;5;241m=\u001b[39msize,\n\u001b[1;32m    409\u001b[0m         do_rescale\u001b[38;5;241m=\u001b[39mdo_rescale,\n\u001b[1;32m    410\u001b[0m         rescale_factor\u001b[38;5;241m=\u001b[39mrescale_factor,\n\u001b[1;32m    411\u001b[0m         do_normalize\u001b[38;5;241m=\u001b[39mdo_normalize,\n\u001b[1;32m    412\u001b[0m         image_mean\u001b[38;5;241m=\u001b[39mimage_mean,\n\u001b[1;32m    413\u001b[0m         image_std\u001b[38;5;241m=\u001b[39mimage_std,\n\u001b[1;32m    414\u001b[0m         data_format\u001b[38;5;241m=\u001b[39mdata_format,\n\u001b[1;32m    415\u001b[0m         input_data_format\u001b[38;5;241m=\u001b[39minput_data_format,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    418\u001b[0m ]\n\u001b[1;32m    420\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m segmentation_maps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:404\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m     )\n\u001b[1;32m    392\u001b[0m validate_preprocess_arguments(\n\u001b[1;32m    393\u001b[0m     do_rescale\u001b[38;5;241m=\u001b[39mdo_rescale,\n\u001b[1;32m    394\u001b[0m     rescale_factor\u001b[38;5;241m=\u001b[39mrescale_factor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m    401\u001b[0m )\n\u001b[1;32m    403\u001b[0m images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_resize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_resize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrescale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    418\u001b[0m ]\n\u001b[1;32m    420\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m segmentation_maps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:243\u001b[0m, in \u001b[0;36mSegformerImageProcessor._preprocess_image\u001b[0;34m(self, image, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     input_data_format \u001b[38;5;241m=\u001b[39m infer_channel_dimension_format(image)\n\u001b[0;32m--> 243\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_reduce_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_resize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_resize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrescale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     image \u001b[38;5;241m=\u001b[39m to_channel_dimension_format(image, data_format, input_channel_dim\u001b[38;5;241m=\u001b[39minput_data_format)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:215\u001b[0m, in \u001b[0;36mSegformerImageProcessor._preprocess\u001b[0;34m(self, image, do_reduce_labels, do_resize, do_rescale, do_normalize, size, resample, rescale_factor, image_mean, image_std, input_data_format)\u001b[0m\n\u001b[1;32m    212\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 215\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/image_processing_utils.py:111\u001b[0m, in \u001b[0;36mBaseImageProcessor.normalize\u001b[0;34m(self, image, mean, std, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize\u001b[39m(\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     80\u001b[0m     image: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     86\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Normalize an image. image = (image - image_mean) / image_std.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m        `np.ndarray`: The normalized image.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/image_transforms.py:409\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(image, mean, std, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     image \u001b[38;5;241m=\u001b[39m ((image\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 409\u001b[0m image \u001b[38;5;241m=\u001b[39m to_channel_dimension_format(image, data_format, input_data_format) \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m image\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = SegformerImageProcessor(\n",
    "            image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "            image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "            do_reduce_labels=False\n",
    "            )\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        dataframe = pd.read_csv(\n",
    "            f\"{root_dir}/orig_palsa_labels.csv\", \n",
    "            names=['filename', 'palsa'], \n",
    "            header=0\n",
    "            )\n",
    "        \n",
    "        dataframe = dataframe.loc[dataframe['palsa']>0]\n",
    "        dataframe = dataframe[~dataframe['filename'].str.endswith('aug')]\n",
    "        checked_names = list(dataframe['filename'])\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f[:-4] in checked_names]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "##############\n",
    "# Custom Loss\n",
    "##############\n",
    "\n",
    "def weighted_cross_entropy_loss(logits, targets, class_weights=[1, 24]):\n",
    "    \"\"\"\n",
    "    Calculate weighted cross-entropy loss for binary segmentation using PyTorch's built-in functions.\n",
    "    \n",
    "    Args:\n",
    "    logits (torch.Tensor): Predicted logits with shape [batch, num_classes, height, width]\n",
    "    targets (torch.Tensor): Ground truth labels with shape [batch, height, width]\n",
    "    class_weights (list): Weights for each class [weight_class_0, weight_class_1]\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Weighted cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    device = logits.device\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    # Convert class weights to a tensor and move to the same device\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Create the loss function with weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "    \n",
    "    # Calculate and return the loss\n",
    "    return criterion(logits, targets)\n",
    "\n",
    "# Example usage:\n",
    "# logits = torch.randn(32, 2, 512, 512)  # [batch, num_classes, height, width]\n",
    "# targets = torch.randint(0, 2, (32, 512, 512))  # [batch, height, width]\n",
    "# loss = weighted_cross_entropy_loss(logits, targets)\n",
    "\n",
    "\n",
    "###################\n",
    "# Generate Datasets\n",
    "###################\n",
    "\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SemanticSegmentationDataset(root_dir)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20)#, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=20)\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b0\", \n",
    "    num_labels=2\n",
    ") \n",
    "\n",
    "# Freeze encoder layers\n",
    "for param in model.segformer.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optionally, unfreeze the last few layers of the encoder\n",
    "# Adjust the number of unfrozen blocks as needed\n",
    "num_unfrozen_blocks = 2\n",
    "for i in range(len(model.segformer.encoder.block) - num_unfrozen_blocks, len(model.segformer.encoder.block)):\n",
    "    for param in model.segformer.encoder.block[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "epochs = 4\n",
    "lr = 0.1\n",
    "warmup_steps = 100  # Adjust this value as needed\n",
    "\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = weighted_cross_entropy_loss\n",
    "\n",
    "\n",
    "# define scheduler\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# run = wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"Finetune_segformer\",\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"epochs\": epochs,\n",
    "#         \"lr\": lr,\n",
    "#         \"warmup_steps\": warmup_steps,\n",
    "#         \"patience\": patience\n",
    "#         }\n",
    "# )\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits.unsqueeze(1).float(), \n",
    "            size=[logits.shape[1],labels.shape[-2],labels.shape[-1]], \n",
    "            mode=\"nearest\")\n",
    "        loss = criterion(upsampled_logits.squeeze(1), labels)\n",
    "\n",
    "        # debugging\n",
    "        # predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "        # print(torch.argmax(logits, dim=1).sum())\n",
    "        # print(f\"loss = {loss}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        # progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    model.eval()\n",
    "    jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Calculate Jaccard score\n",
    "            # Convert logits to binary segmentation mask\n",
    "            predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "            \n",
    "            # Upsample the predicted mask to match the label size\n",
    "            upsampled_predicted = F.interpolate(predicted.unsqueeze(1).float(), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            jaccard = jaccard_index(upsampled_predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2)\n",
    "            jaccard_scores.append(jaccard)\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for target class only, if not a only background image\n",
    "            if len(labels.unique()) > 1:\n",
    "                target_jaccard = jaccard_index(upsampled_predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2, average=\"none\")[1]\n",
    "                target_jaccard_scores.append(target_jaccard)\n",
    "\n",
    "    avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    # wandb.log({\"jaccard\": avg_jaccard})\n",
    "    # wandb.log({\"target_jaccard\": avg_target_jaccard})\n",
    "    print(f\"Epoch {epoch}, Average Jaccard Score: {avg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    # # Early stopping check based on target Jaccard score\n",
    "    # if avg_jaccard > best_jaccard:\n",
    "    #     best_jaccard = avg_jaccard\n",
    "    #     epochs_no_improve = 0\n",
    "    #     # Save the best model\n",
    "    #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # else:\n",
    "    #     epochs_no_improve += 1\n",
    "    #     if epochs_no_improve == patience:\n",
    "    #         print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "    #         break\n",
    "    \n",
    "\n",
    "\n",
    "# artifact = wandb.Artifact('finetuned_segformer', type='model')\n",
    "# artifact.add_file('best_model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The run below worked surprisingly well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-ade-640-640 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([2, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 85/85 [00:56<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Jaccard Score: 0.3037, Target Class Jaccard Score: 0.0608\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 85/85 [00:57<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Jaccard Score: 0.3288, Target Class Jaccard Score: 0.0593\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 85/85 [00:56<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Jaccard Score: 0.2645, Target Class Jaccard Score: 0.0625\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 85/85 [00:56<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Jaccard Score: 0.3176, Target Class Jaccard Score: 0.0575\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = SegformerImageProcessor(\n",
    "            image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "            image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "            do_reduce_labels=False\n",
    "            )\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        dataframe = pd.read_csv(\n",
    "            f\"{root_dir}/orig_palsa_labels.csv\", \n",
    "            names=['filename', 'palsa'], \n",
    "            header=0\n",
    "            )\n",
    "        \n",
    "        dataframe = dataframe.loc[dataframe['palsa']>0]\n",
    "        dataframe = dataframe[~dataframe['filename'].str.endswith('aug')]\n",
    "        checked_names = list(dataframe['filename'])\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f[:-4] in checked_names][:500]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "##############\n",
    "# Custom Loss\n",
    "##############\n",
    "\n",
    "def weighted_cross_entropy_loss(logits, targets, class_weights=[1, 6]): # shuld be 1,24\n",
    "    \"\"\"\n",
    "    Calculate weighted cross-entropy loss for binary segmentation using PyTorch's built-in functions.\n",
    "    \n",
    "    Args:\n",
    "    logits (torch.Tensor): Predicted logits with shape [batch, num_classes, height, width]\n",
    "    targets (torch.Tensor): Ground truth labels with shape [batch, height, width]\n",
    "    class_weights (list): Weights for each class [weight_class_0, weight_class_1]\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Weighted cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    device = logits.device\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    # Convert class weights to a tensor and move to the same device\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Create the loss function with weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "    \n",
    "    # Calculate and return the loss\n",
    "    return criterion(logits, targets)\n",
    "\n",
    "# Example usage:\n",
    "# logits = torch.randn(32, 2, 512, 512)  # [batch, num_classes, height, width]\n",
    "# targets = torch.randint(0, 2, (32, 512, 512))  # [batch, height, width]\n",
    "# loss = weighted_cross_entropy_loss(logits, targets)\n",
    "\n",
    "\n",
    "#########\n",
    "# CONFIGS\n",
    "#########\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 4\n",
    "batch_size = 5\n",
    "lr = 7e-7 # in satellite segformer used 7e-5\n",
    "warmup_steps = 100  # Adjust this value as needed\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# run = wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"Finetune_segformer\",\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"epochs\": epochs,\n",
    "#         \"lr\": lr,\n",
    "#         \"warmup_steps\": warmup_steps,\n",
    "#         \"patience\": patience\n",
    "#         }\n",
    "# )\n",
    "\n",
    "###################\n",
    "# Generate Datasets\n",
    "###################\n",
    "\n",
    "# Create the full dataset\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "full_dataset = SemanticSegmentationDataset(root_dir)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)#, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b5-finetuned-ade-640-640\",\n",
    "    # \"sawthiha/segformer-b0-finetuned-deprem-satellite\", \n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ") \n",
    "\n",
    "# Set learnable layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# # Freeze encoder layers\n",
    "# for param in model.segformer.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Optionally, unfreeze the last few layers of the encoder\n",
    "# # Adjust the number of unfrozen blocks as needed\n",
    "# num_unfrozen_blocks = 4\n",
    "# for i in range(len(model.segformer.encoder.block) - num_unfrozen_blocks, len(model.segformer.encoder.block)):\n",
    "#     for param in model.segformer.encoder.block[i].parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# model to device\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay = 0.03)\n",
    "\n",
    "criterion = weighted_cross_entropy_loss\n",
    "\n",
    "# define scheduler\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits.unsqueeze(1).float(), \n",
    "            size=[logits.shape[1],labels.shape[-2],labels.shape[-1]], \n",
    "            mode=\"nearest\")\n",
    "        loss = criterion(upsampled_logits.squeeze(1), labels)\n",
    "\n",
    "        # debugging\n",
    "        # predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "        # print(torch.argmax(logits, dim=1).sum())\n",
    "        # print(f\"loss = {loss}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        # progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    model.eval()\n",
    "    jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Calculate Jaccard score\n",
    "            # Convert logits to binary segmentation mask\n",
    "            predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "            \n",
    "            # Upsample the predicted mask to match the label size\n",
    "            upsampled_predicted = F.interpolate(predicted.unsqueeze(1).float(), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            jaccard = jaccard_index(upsampled_predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2)\n",
    "            jaccard_scores.append(jaccard)\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for target class only, if not a only background image\n",
    "            if len(labels.unique()) > 1:\n",
    "                target_jaccard = jaccard_index(upsampled_predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2, average=\"none\")[1]\n",
    "                target_jaccard_scores.append(target_jaccard)\n",
    "\n",
    "    avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    # wandb.log({\"jaccard\": avg_jaccard})\n",
    "    # wandb.log({\"target_jaccard\": avg_target_jaccard})\n",
    "    print(f\"Epoch {epoch}, Average Jaccard Score: {avg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    # # Early stopping check based on target Jaccard score\n",
    "    # if avg_jaccard > best_jaccard:\n",
    "    #     best_jaccard = avg_jaccard\n",
    "    #     epochs_no_improve = 0\n",
    "    #     # Save the best model\n",
    "    #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # else:\n",
    "    #     epochs_no_improve += 1\n",
    "    #     if epochs_no_improve == patience:\n",
    "    #         print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "    #         break\n",
    "    \n",
    "\n",
    "\n",
    "# artifact = wandb.Artifact('finetuned_segformer', type='model')\n",
    "# artifact.add_file('best_model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-class + background model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cross_entropy_loss(logits, targets, class_weights=[1, 6]): # shuld be 1,24\n",
    "    \"\"\"\n",
    "    Calculate weighted cross-entropy loss for binary segmentation using PyTorch's built-in functions.\n",
    "    \n",
    "    Args:\n",
    "    logits (torch.Tensor): Predicted logits with shape [batch, num_classes, height, width]\n",
    "    targets (torch.Tensor): Ground truth labels with shape [batch, height, width]\n",
    "    class_weights (list): Weights for each class [weight_class_0, weight_class_1]\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Weighted cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    device = logits.device\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    # Convert class weights to a tensor and move to the same device\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Create the loss function with weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "    \n",
    "    # Calculate and return the loss\n",
    "    return criterion(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-ade-640-640 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([2, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:   0%|          | 0/85 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input and target batch or spatial sizes don't match: target [5, 512, 512], input [5, 2, 128, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 168\u001b[0m\n\u001b[1;32m    165\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# scheduler.step()  # Update learning rate\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     customloss \u001b[38;5;241m=\u001b[39m \u001b[43mweighted_cross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# Update progress bar\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\u001b[39;00m\n\u001b[1;32m    173\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mweighted_cross_entropy_loss\u001b[0;34m(logits, targets, class_weights)\u001b[0m\n\u001b[1;32m     21\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mclass_weights, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Calculate and return the loss\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input and target batch or spatial sizes don't match: target [5, 512, 512], input [5, 2, 128, 128]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = SegformerImageProcessor(\n",
    "            image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "            image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "            do_reduce_labels=False\n",
    "            )\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        dataframe = pd.read_csv(\n",
    "            f\"{root_dir}/orig_palsa_labels.csv\", \n",
    "            names=['filename', 'palsa'], \n",
    "            header=0\n",
    "            )\n",
    "        \n",
    "        dataframe = dataframe.loc[dataframe['palsa']>0]\n",
    "        dataframe = dataframe[~dataframe['filename'].str.endswith('aug')]\n",
    "        checked_names = list(dataframe['filename'])\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f[:-4] in checked_names][:500] # TODO REMOVE THE \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "\n",
    "#########\n",
    "# CONFIGS\n",
    "#########\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 4\n",
    "batch_size = 5\n",
    "lr = 7e-7 # in satellite segformer used 7e-5\n",
    "warmup_steps = 100  # Adjust this value as needed\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# run = wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"Finetune_segformer\",\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"epochs\": epochs,\n",
    "#         \"lr\": lr,\n",
    "#         \"warmup_steps\": warmup_steps,\n",
    "#         \"patience\": patience\n",
    "#         }\n",
    "# )\n",
    "\n",
    "###################\n",
    "# Generate Datasets\n",
    "###################\n",
    "\n",
    "# Create the full dataset\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "full_dataset = SemanticSegmentationDataset(root_dir)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)#, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b5-finetuned-ade-640-640\",\n",
    "    # \"sawthiha/segformer-b0-finetuned-deprem-satellite\", \n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ") \n",
    "\n",
    "# Set learnable layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# model to device\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay = 0.03)\n",
    "\n",
    "# define scheduler\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "\n",
    "#########\n",
    "# TRAIN #\n",
    "#########\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        # progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    model.eval()\n",
    "    bg_jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Upsample the predicted mask to match the label size\n",
    "            upsampled_logits = F.interpolate(logits.float(), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            jaccard = jaccard_index(upsampled_logits, labels, task=\"multiclass\", num_classes=2, average= 'none')\n",
    "            bg_jaccard_scores.append(jaccard[0])\n",
    "            target_jaccard_scores.append(jaccard[1])\n",
    "\n",
    "    avg_bg_jaccard = sum(bg_jaccard_scores) / len(bg_jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    # wandb.log({\"background_jaccard\": avg_bg_jaccard})\n",
    "    # wandb.log({\"target_jaccard\": avg_target_jaccard})\n",
    "    print(f\"Epoch {epoch}, Average Background Jaccard Score: {avg_bg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    # # Early stopping check based on target Jaccard score\n",
    "    # if avg_jaccard > best_jaccard:\n",
    "    #     best_jaccard = avg_jaccard\n",
    "    #     epochs_no_improve = 0\n",
    "    #     # Save the best model\n",
    "    #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # else:\n",
    "    #     epochs_no_improve += 1\n",
    "    #     if epochs_no_improve == patience:\n",
    "    #         print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "    #         break\n",
    "    \n",
    "# artifact = wandb.Artifact('finetuned_segformer', type='model')\n",
    "# artifact.add_file('best_model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall info\n",
    "\n",
    "Folder structure will be: \n",
    "\n",
    "Dataset \n",
    "    |__RGB \n",
    "    |__HS \n",
    "    |__DEM \n",
    "    |__annotations \n",
    "    |__labels.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe\n",
    "- All of the below script has been written but not tested \n",
    "- to run it, several installs might be needed. \n",
    "\n",
    "##### Useful links: \n",
    "- [source code](https://github.com/huggingface/transformers/blob/main/src/transformers/models/segformer/image_processing_segformer.py)\n",
    "- https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, image_processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "    \n",
    "\"\"\"\n",
    "WARNING: \n",
    "by default the image processor below will resize the image (to 512*512).\n",
    "Essentially i don't want this, HOWEVER it might be necessary to be able\n",
    "to use the weights from pretraining. TODO: Find out whether that's so.\n",
    "\"\"\"\n",
    "\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "image_processor = SegformerImageProcessor(\n",
    "    image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "    image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "    do_reduce_labels=False\n",
    "    )\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SemanticSegmentationDataset(root_dir, image_processor)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "### IMPORTANT: DO I WANT TO FREEZE LAYERS??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b0\", \n",
    "    num_labels=1# since we treat '0' as a background, the only class is palsa.\n",
    ") \n",
    "\n",
    "# Freeze encoder layers\n",
    "for param in model.segformer.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optionally, unfreeze the last few layers of the encoder\n",
    "# Adjust the number of unfrozen blocks as needed\n",
    "num_unfrozen_blocks = 2\n",
    "for i in range(len(model.segformer.encoder.block) - num_unfrozen_blocks, len(model.segformer.encoder.block)):\n",
    "    for param in model.segformer.encoder.block[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# The decoder (model.decode_head) will be trained by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the size of next iteration of valid loader image and labels, as well as last model output size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune\n",
    "based on [huggingface tutorial](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb)\n",
    "\n",
    "DO i want to log all of this with wandb?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 194/194 [16:30<00:00,  5.11s/it, Loss=0.0190, LR=0.000059]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [128] and output size of torch.Size([512, 512]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Calculate Jaccard score\u001b[39;00m\n\u001b[1;32m     71\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# chooses class with highest probability per \u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m upsampled_logits \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Calculate overall Jaccard score (including background)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m jaccard \u001b[38;5;241m=\u001b[39m jaccard_index(upsampled_logits, labels, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/nn/functional.py:3983\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   3982\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m-> 3983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3984\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3985\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput with spatial dimensions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3986\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3987\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3988\u001b[0m         )\n\u001b[1;32m   3989\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m   3990\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_integer(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m size):\n",
      "\u001b[0;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [128] and output size of torch.Size([512, 512]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "\n",
    "epochs = 20\n",
    "lr = 0.00006\n",
    "warmup_steps = 100  # Adjust this value as needed\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# define scheduler\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    model.eval()\n",
    "    jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Calculate Jaccard score\n",
    "            # Since we only have one feature map, we can use a threshold to determine the segmentation\n",
    "            predicted = (logits.squeeze(1) > 0).float()  # Threshold at 0\n",
    "            upsampled_predicted = F.interpolate(predicted.unsqueeze(1), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            jaccard = jaccard_index(upsampled_predicted.squeeze(1), labels, task=\"multiclass\", num_classes=2)\n",
    "            jaccard_scores.append(jaccard.item())\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for target class only, if not a only background image\n",
    "            if len(labels.unique()) > 1:\n",
    "                target_jaccard = jaccard_index(upsampled_predicted.squeeze(1), labels, task=\"multiclass\", num_classes=2, average=\"none\")[1]\n",
    "                target_jaccard_scores.append(target_jaccard.item())\n",
    "\n",
    "    avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    print(f\"Epoch {epoch}, Average Jaccard Score: {avg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    # Early stopping check based on target Jaccard score\n",
    "    if avg_jaccard > best_jaccard:\n",
    "        best_jaccard = avg_jaccard\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "            break\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "# Load the best model after training\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a result with the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('imgpath')\n",
    "pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values=pixel_values)\n",
    "logits = outputs.logits.cpu()\n",
    "predicted_segmentation_map = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
    "predicted_segmentation_map = predicted_segmentation_map.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "color_seg = np.zeros((predicted_segmentation_map.shape[0],\n",
    "                      predicted_segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "\n",
    "color = np.array([4, 250, 7])\n",
    "color_seg[predicted_segmentation_map == 0, :] = color\n",
    "# Convert to BGR\n",
    "color_seg = color_seg[..., ::-1]\n",
    "\n",
    "# Show image + mask\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palsa_env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
