{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-ade-640-640 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([2, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/8: 100%|██████████| 51/51 [00:23<00:00,  2.14it/s, Loss=0.6402, LR=0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Background Jaccard Score: 0.7886, Target Class Jaccard Score: 0.0446\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/8: 100%|██████████| 51/51 [00:22<00:00,  2.24it/s, Loss=0.6562, LR=0.000001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Background Jaccard Score: 0.8244, Target Class Jaccard Score: 0.0382\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/8: 100%|██████████| 51/51 [00:23<00:00,  2.18it/s, Loss=0.6687, LR=0.000001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Background Jaccard Score: 0.8572, Target Class Jaccard Score: 0.0311\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/8: 100%|██████████| 51/51 [00:23<00:00,  2.15it/s, Loss=0.6525, LR=0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Background Jaccard Score: 0.6763, Target Class Jaccard Score: 0.0555\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/8: 100%|██████████| 51/51 [00:22<00:00,  2.23it/s, Loss=0.6603, LR=0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Average Background Jaccard Score: 0.7553, Target Class Jaccard Score: 0.0482\n",
      "Epoch: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/8: 100%|██████████| 51/51 [00:23<00:00,  2.17it/s, Loss=0.6497, LR=0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Average Background Jaccard Score: 0.7868, Target Class Jaccard Score: 0.0447\n",
      "Epoch: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/8: 100%|██████████| 51/51 [00:24<00:00,  2.08it/s, Loss=0.6712, LR=0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Average Background Jaccard Score: 0.7769, Target Class Jaccard Score: 0.0445\n",
      "Epoch: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/8: 100%|██████████| 51/51 [00:24<00:00,  2.11it/s, Loss=0.6650, LR=0.000000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Average Background Jaccard Score: 0.7465, Target Class Jaccard Score: 0.0482\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = SegformerImageProcessor(\n",
    "            image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "            image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "            do_reduce_labels=False\n",
    "            )\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        dataframe = pd.read_csv(\n",
    "            f\"{root_dir}/orig_palsa_labels.csv\", \n",
    "            names=['filename', 'palsa'], \n",
    "            header=0\n",
    "            )\n",
    "        \n",
    "        dataframe = dataframe.loc[dataframe['palsa']>0]\n",
    "        dataframe = dataframe[~dataframe['filename'].str.endswith('aug')]\n",
    "        checked_names = list(dataframe['filename'])\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f[:-4] in checked_names][:300]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "##############\n",
    "# Custom Loss\n",
    "##############\n",
    "\n",
    "def weighted_cross_entropy_loss(logits, targets, class_weights=[1, 6]): # shuld be 1,24\n",
    "    \"\"\"\n",
    "    Calculate weighted cross-entropy loss for binary segmentation using PyTorch's built-in functions.\n",
    "    \n",
    "    Args:\n",
    "    logits (torch.Tensor): Predicted logits with shape [batch, num_classes, height, width]\n",
    "    targets (torch.Tensor): Ground truth labels with shape [batch, height, width]\n",
    "    class_weights (list): Weights for each class [weight_class_0, weight_class_1]\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Weighted cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    device = logits.device\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    # Convert class weights to a tensor and move to the same device\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Create the loss function with weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "    \n",
    "    # Calculate and return the loss\n",
    "    return criterion(logits, targets)\n",
    "\n",
    "# Example usage:\n",
    "# logits = torch.randn(32, 2, 512, 512)  # [batch, num_classes, height, width]\n",
    "# targets = torch.randint(0, 2, (32, 512, 512))  # [batch, height, width]\n",
    "# loss = weighted_cross_entropy_loss(logits, targets)\n",
    "\n",
    "\n",
    "#########\n",
    "# CONFIGS\n",
    "#########\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 8\n",
    "batch_size = 5\n",
    "lr = 7e-7 # in satellite segformer used 7e-5\n",
    "warmup_steps = 100 # Adjust this value as needed\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "freeze_encoder = True\n",
    "# model_name = \"sawthiha/segformer-b0-finetuned-deprem-satellite\"\n",
    "model_name = \"nvidia/segformer-b5-finetuned-ade-640-640\"\n",
    "\n",
    "# run = wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"Finetune_segformer\",\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"epochs\": epochs,\n",
    "#         \"lr\": lr,\n",
    "#         \"warmup_steps\": warmup_steps,\n",
    "#         \"patience\": patience,\n",
    "#         \"freeze_encoder\": freeze_encoder,\n",
    "#         \"model_name\": model_name\n",
    "#         }\n",
    "# )\n",
    "\n",
    "###################\n",
    "# Generate Datasets\n",
    "###################\n",
    "\n",
    "# Create the full dataset\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "full_dataset = SemanticSegmentationDataset(root_dir)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)#, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ") \n",
    "\n",
    "# Set learnable layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Freeze encoder layers\n",
    "if freeze_encoder == True:\n",
    "    for param in model.segformer.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "# # Optionally, unfreeze the last few layers of the encoder\n",
    "# # Adjust the number of unfrozen blocks as needed\n",
    "# num_unfrozen_blocks = 4\n",
    "# for i in range(len(model.segformer.encoder.block) - num_unfrozen_blocks, len(model.segformer.encoder.block)):\n",
    "#     for param in model.segformer.encoder.block[i].parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# model to device\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay = 0.03)\n",
    "\n",
    "criterion = weighted_cross_entropy_loss\n",
    "\n",
    "# define scheduler\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    train_loss = []\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits.unsqueeze(1).float(), \n",
    "            size=[logits.shape[1],labels.shape[-2],labels.shape[-1]], \n",
    "            mode=\"nearest\")\n",
    "        loss = criterion(upsampled_logits.squeeze(1), labels)\n",
    "        train_loss.append(loss.detach().cpu())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    avg_train_loss = sum(train_loss) / len(train_loss)\n",
    "    # wandb.log({\"train_loss\": avg_train_loss})\n",
    "\n",
    "    model.eval()\n",
    "    bg_jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    val_loss = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "            val_loss.append(loss.detach().cpu())\n",
    "\n",
    "            # Convert logits to binary segmentation mask\n",
    "            predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "            \n",
    "            # Upsample the predicted mask to match the label size\n",
    "            upsampled_predicted = F.interpolate(predicted.unsqueeze(1).float(), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            jaccard = jaccard_index(upsampled_predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2, average='none')\n",
    "            bg_jaccard_scores.append(jaccard[0])\n",
    "            target_jaccard_scores.append(jaccard[1])\n",
    "\n",
    "        avg_val_loss = sum(val_loss) / len(val_loss)\n",
    "        # wandb.log({\"val_loss\": avg_val_loss})\n",
    "\n",
    "    avg_bg_jaccard = sum(bg_jaccard_scores) / len(bg_jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    # wandb.log({\"background_jaccard\": avg_bg_jaccard})\n",
    "    # wandb.log({\"target_jaccard\": avg_target_jaccard})\n",
    "    print(f\"Epoch {epoch}, Average Background Jaccard Score: {avg_bg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    # # Early stopping check based on target Jaccard score\n",
    "    # if avg_jaccard > best_jaccard:\n",
    "    #     best_jaccard = avg_jaccard\n",
    "    #     epochs_no_improve = 0\n",
    "    #     # Save the best model\n",
    "    #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # else:\n",
    "    #     epochs_no_improve += 1\n",
    "    #     if epochs_no_improve == patience:\n",
    "    #         print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "    #         break\n",
    "    \n",
    "\n",
    "# artifact = wandb.Artifact('finetuned_segformer', type='model')\n",
    "# artifact.add_file('best_model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The run below worked surprisingly well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-ade-640-640 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([2, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 85/85 [00:56<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Jaccard Score: 0.3037, Target Class Jaccard Score: 0.0608\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 85/85 [00:57<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Jaccard Score: 0.3288, Target Class Jaccard Score: 0.0593\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 85/85 [00:56<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Jaccard Score: 0.2645, Target Class Jaccard Score: 0.0625\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 85/85 [00:56<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Jaccard Score: 0.3176, Target Class Jaccard Score: 0.0575\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = SegformerImageProcessor(\n",
    "            image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "            image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "            do_reduce_labels=False\n",
    "            )\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        dataframe = pd.read_csv(\n",
    "            f\"{root_dir}/orig_palsa_labels.csv\", \n",
    "            names=['filename', 'palsa'], \n",
    "            header=0\n",
    "            )\n",
    "        \n",
    "        dataframe = dataframe.loc[dataframe['palsa']>0]\n",
    "        dataframe = dataframe[~dataframe['filename'].str.endswith('aug')]\n",
    "        checked_names = list(dataframe['filename'])\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f[:-4] in checked_names][:500]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "##############\n",
    "# Custom Loss\n",
    "##############\n",
    "\n",
    "def weighted_cross_entropy_loss(logits, targets, class_weights=[1, 6]): # shuld be 1,24\n",
    "    \"\"\"\n",
    "    Calculate weighted cross-entropy loss for binary segmentation using PyTorch's built-in functions.\n",
    "    \n",
    "    Args:\n",
    "    logits (torch.Tensor): Predicted logits with shape [batch, num_classes, height, width]\n",
    "    targets (torch.Tensor): Ground truth labels with shape [batch, height, width]\n",
    "    class_weights (list): Weights for each class [weight_class_0, weight_class_1]\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Weighted cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    device = logits.device\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    # Convert class weights to a tensor and move to the same device\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Create the loss function with weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "    \n",
    "    # Calculate and return the loss\n",
    "    return criterion(logits, targets)\n",
    "\n",
    "# Example usage:\n",
    "# logits = torch.randn(32, 2, 512, 512)  # [batch, num_classes, height, width]\n",
    "# targets = torch.randint(0, 2, (32, 512, 512))  # [batch, height, width]\n",
    "# loss = weighted_cross_entropy_loss(logits, targets)\n",
    "\n",
    "\n",
    "#########\n",
    "# CONFIGS\n",
    "#########\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 4\n",
    "batch_size = 5\n",
    "lr = 7e-7 # in satellite segformer used 7e-5\n",
    "warmup_steps = 100  # Adjust this value as needed\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# run = wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"Finetune_segformer\",\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"epochs\": epochs,\n",
    "#         \"lr\": lr,\n",
    "#         \"warmup_steps\": warmup_steps,\n",
    "#         \"patience\": patience\n",
    "#         }\n",
    "# )\n",
    "\n",
    "###################\n",
    "# Generate Datasets\n",
    "###################\n",
    "\n",
    "# Create the full dataset\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "full_dataset = SemanticSegmentationDataset(root_dir)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)#, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b5-finetuned-ade-640-640\",\n",
    "    # \"sawthiha/segformer-b0-finetuned-deprem-satellite\", \n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ") \n",
    "\n",
    "# Set learnable layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# # Freeze encoder layers\n",
    "# for param in model.segformer.encoder.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "# # Optionally, unfreeze the last few layers of the encoder\n",
    "# # Adjust the number of unfrozen blocks as needed\n",
    "# num_unfrozen_blocks = 4\n",
    "# for i in range(len(model.segformer.encoder.block) - num_unfrozen_blocks, len(model.segformer.encoder.block)):\n",
    "#     for param in model.segformer.encoder.block[i].parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# model to device\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay = 0.03)\n",
    "\n",
    "criterion = weighted_cross_entropy_loss\n",
    "\n",
    "# define scheduler\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits.unsqueeze(1).float(), \n",
    "            size=[logits.shape[1],labels.shape[-2],labels.shape[-1]], \n",
    "            mode=\"nearest\")\n",
    "        loss = criterion(upsampled_logits.squeeze(1), labels)\n",
    "\n",
    "        # debugging\n",
    "        # predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "        # print(torch.argmax(logits, dim=1).sum())\n",
    "        # print(f\"loss = {loss}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        # progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    model.eval()\n",
    "    jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Calculate Jaccard score\n",
    "            # Convert logits to binary segmentation mask\n",
    "            predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "            \n",
    "            # Upsample the predicted mask to match the label size\n",
    "            upsampled_predicted = F.interpolate(predicted.unsqueeze(1).float(), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            jaccard = jaccard_index(upsampled_predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2)\n",
    "            jaccard_scores.append(jaccard)\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for target class only, if not a only background image\n",
    "            if len(labels.unique()) > 1:\n",
    "                target_jaccard = jaccard_index(upsampled_predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2, average=\"none\")[1]\n",
    "                target_jaccard_scores.append(target_jaccard)\n",
    "\n",
    "    avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    # wandb.log({\"jaccard\": avg_jaccard})\n",
    "    # wandb.log({\"target_jaccard\": avg_target_jaccard})\n",
    "    print(f\"Epoch {epoch}, Average Jaccard Score: {avg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    # # Early stopping check based on target Jaccard score\n",
    "    # if avg_jaccard > best_jaccard:\n",
    "    #     best_jaccard = avg_jaccard\n",
    "    #     epochs_no_improve = 0\n",
    "    #     # Save the best model\n",
    "    #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # else:\n",
    "    #     epochs_no_improve += 1\n",
    "    #     if epochs_no_improve == patience:\n",
    "    #         print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "    #         break\n",
    "    \n",
    "\n",
    "\n",
    "# artifact = wandb.Artifact('finetuned_segformer', type='model')\n",
    "# artifact.add_file('best_model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-class + background model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_cross_entropy_loss(logits, targets, class_weights=[1, 6]): # shuld be 1,24\n",
    "    \"\"\"\n",
    "    Calculate weighted cross-entropy loss for binary segmentation using PyTorch's built-in functions.\n",
    "    \n",
    "    Args:\n",
    "    logits (torch.Tensor): Predicted logits with shape [batch, num_classes, height, width]\n",
    "    targets (torch.Tensor): Ground truth labels with shape [batch, height, width]\n",
    "    class_weights (list): Weights for each class [weight_class_0, weight_class_1]\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Weighted cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    device = logits.device\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    # Convert class weights to a tensor and move to the same device\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Create the loss function with weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "    \n",
    "    # Calculate and return the loss\n",
    "    return criterion(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-ade-640-640 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([2, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:   0%|          | 0/85 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input and target batch or spatial sizes don't match: target [5, 512, 512], input [5, 2, 128, 128]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 168\u001b[0m\n\u001b[1;32m    165\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# scheduler.step()  # Update learning rate\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m     customloss \u001b[38;5;241m=\u001b[39m \u001b[43mweighted_cross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# Update progress bar\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\u001b[39;00m\n\u001b[1;32m    173\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mweighted_cross_entropy_loss\u001b[0;34m(logits, targets, class_weights)\u001b[0m\n\u001b[1;32m     21\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(weight\u001b[38;5;241m=\u001b[39mclass_weights, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Calculate and return the loss\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/nn/modules/loss.py:1188\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1189\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1190\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/nn/functional.py:3104\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3103\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input and target batch or spatial sizes don't match: target [5, 512, 512], input [5, 2, 128, 128]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = SegformerImageProcessor(\n",
    "            image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "            image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "            do_reduce_labels=False\n",
    "            )\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        dataframe = pd.read_csv(\n",
    "            f\"{root_dir}/orig_palsa_labels.csv\", \n",
    "            names=['filename', 'palsa'], \n",
    "            header=0\n",
    "            )\n",
    "        \n",
    "        dataframe = dataframe.loc[dataframe['palsa']>0]\n",
    "        dataframe = dataframe[~dataframe['filename'].str.endswith('aug')]\n",
    "        checked_names = list(dataframe['filename'])\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f[:-4] in checked_names][:500] # TODO REMOVE THE \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "\n",
    "#########\n",
    "# CONFIGS\n",
    "#########\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 4\n",
    "batch_size = 5\n",
    "lr = 7e-7 # in satellite segformer used 7e-5\n",
    "warmup_steps = 100  # Adjust this value as needed\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# run = wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"Finetune_segformer\",\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"epochs\": epochs,\n",
    "#         \"lr\": lr,\n",
    "#         \"warmup_steps\": warmup_steps,\n",
    "#         \"patience\": patience\n",
    "#         }\n",
    "# )\n",
    "\n",
    "###################\n",
    "# Generate Datasets\n",
    "###################\n",
    "\n",
    "# Create the full dataset\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "full_dataset = SemanticSegmentationDataset(root_dir)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)#, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b5-finetuned-ade-640-640\",\n",
    "    # \"sawthiha/segformer-b0-finetuned-deprem-satellite\", \n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ") \n",
    "\n",
    "# Set learnable layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# model to device\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay = 0.03)\n",
    "\n",
    "# define scheduler\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "\n",
    "#########\n",
    "# TRAIN #\n",
    "#########\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        # progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    model.eval()\n",
    "    bg_jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Upsample the predicted mask to match the label size\n",
    "            upsampled_logits = F.interpolate(logits.float(), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            jaccard = jaccard_index(upsampled_logits, labels, task=\"multiclass\", num_classes=2, average= 'none')\n",
    "            bg_jaccard_scores.append(jaccard[0])\n",
    "            target_jaccard_scores.append(jaccard[1])\n",
    "\n",
    "    avg_bg_jaccard = sum(bg_jaccard_scores) / len(bg_jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    # wandb.log({\"background_jaccard\": avg_bg_jaccard})\n",
    "    # wandb.log({\"target_jaccard\": avg_target_jaccard})\n",
    "    print(f\"Epoch {epoch}, Average Background Jaccard Score: {avg_bg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    # # Early stopping check based on target Jaccard score\n",
    "    # if avg_jaccard > best_jaccard:\n",
    "    #     best_jaccard = avg_jaccard\n",
    "    #     epochs_no_improve = 0\n",
    "    #     # Save the best model\n",
    "    #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # else:\n",
    "    #     epochs_no_improve += 1\n",
    "    #     if epochs_no_improve == patience:\n",
    "    #         print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "    #         break\n",
    "    \n",
    "# artifact = wandb.Artifact('finetuned_segformer', type='model')\n",
    "# artifact.add_file('best_model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall info\n",
    "\n",
    "Folder structure will be: \n",
    "\n",
    "Dataset \n",
    "    |__RGB \n",
    "    |__HS \n",
    "    |__DEM \n",
    "    |__annotations \n",
    "    |__labels.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe\n",
    "- All of the below script has been written but not tested \n",
    "- to run it, several installs might be needed. \n",
    "\n",
    "##### Useful links: \n",
    "- [source code](https://github.com/huggingface/transformers/blob/main/src/transformers/models/segformer/image_processing_segformer.py)\n",
    "- https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, image_processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "    \n",
    "\"\"\"\n",
    "WARNING: \n",
    "by default the image processor below will resize the image (to 512*512).\n",
    "Essentially i don't want this, HOWEVER it might be necessary to be able\n",
    "to use the weights from pretraining. TODO: Find out whether that's so.\n",
    "\"\"\"\n",
    "\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "image_processor = SegformerImageProcessor(\n",
    "    image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "    image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "    do_reduce_labels=False\n",
    "    )\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SemanticSegmentationDataset(root_dir, image_processor)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "### IMPORTANT: DO I WANT TO FREEZE LAYERS??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b0\", \n",
    "    num_labels=1# since we treat '0' as a background, the only class is palsa.\n",
    ") \n",
    "\n",
    "# Freeze encoder layers\n",
    "for param in model.segformer.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optionally, unfreeze the last few layers of the encoder\n",
    "# Adjust the number of unfrozen blocks as needed\n",
    "num_unfrozen_blocks = 2\n",
    "for i in range(len(model.segformer.encoder.block) - num_unfrozen_blocks, len(model.segformer.encoder.block)):\n",
    "    for param in model.segformer.encoder.block[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# The decoder (model.decode_head) will be trained by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the size of next iteration of valid loader image and labels, as well as last model output size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune\n",
    "based on [huggingface tutorial](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb)\n",
    "\n",
    "DO i want to log all of this with wandb?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 194/194 [16:30<00:00,  5.11s/it, Loss=0.0190, LR=0.000059]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [128] and output size of torch.Size([512, 512]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Calculate Jaccard score\u001b[39;00m\n\u001b[1;32m     71\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# chooses class with highest probability per \u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m upsampled_logits \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Calculate overall Jaccard score (including background)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m jaccard \u001b[38;5;241m=\u001b[39m jaccard_index(upsampled_logits, labels, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/nn/functional.py:3983\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   3982\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m-> 3983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3984\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3985\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput with spatial dimensions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3986\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3987\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3988\u001b[0m         )\n\u001b[1;32m   3989\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m   3990\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_integer(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m size):\n",
      "\u001b[0;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [128] and output size of torch.Size([512, 512]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "\n",
    "epochs = 20\n",
    "lr = 0.00006\n",
    "warmup_steps = 100  # Adjust this value as needed\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# define scheduler\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    model.eval()\n",
    "    jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Calculate Jaccard score\n",
    "            # Since we only have one feature map, we can use a threshold to determine the segmentation\n",
    "            predicted = (logits.squeeze(1) > 0).float()  # Threshold at 0\n",
    "            upsampled_predicted = F.interpolate(predicted.unsqueeze(1), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            jaccard = jaccard_index(upsampled_predicted.squeeze(1), labels, task=\"multiclass\", num_classes=2)\n",
    "            jaccard_scores.append(jaccard.item())\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for target class only, if not a only background image\n",
    "            if len(labels.unique()) > 1:\n",
    "                target_jaccard = jaccard_index(upsampled_predicted.squeeze(1), labels, task=\"multiclass\", num_classes=2, average=\"none\")[1]\n",
    "                target_jaccard_scores.append(target_jaccard.item())\n",
    "\n",
    "    avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    print(f\"Epoch {epoch}, Average Jaccard Score: {avg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    # Early stopping check based on target Jaccard score\n",
    "    if avg_jaccard > best_jaccard:\n",
    "        best_jaccard = avg_jaccard\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "            break\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "# Load the best model after training\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a result with the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = wandb.Api()\n",
    "\n",
    "artifact_path = f'nadjaflechner/Finetune_segformer_sweep/finetuned_segformer:v44'\n",
    "artifact = api.artifact(artifact_path)\n",
    "artifact_dir = artifact.download()\n",
    "state_dict = torch.load(f\"{artifact_dir}/model.pth\", map_location=torch.device('cpu'))\n",
    "model = model_4D()\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('imgpath')\n",
    "pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values=pixel_values)\n",
    "logits = outputs.logits.cpu()\n",
    "predicted_segmentation_map = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
    "predicted_segmentation_map = predicted_segmentation_map.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "color_seg = np.zeros((predicted_segmentation_map.shape[0],\n",
    "                      predicted_segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "\n",
    "color = np.array([4, 250, 7])\n",
    "color_seg[predicted_segmentation_map == 0, :] = color\n",
    "# Convert to BGR\n",
    "color_seg = color_seg[..., ::-1]\n",
    "\n",
    "# Show image + mask\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focal loss (claude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def focal_loss(inputs, targets, alpha=0.8, gamma=2, reduction='mean'):\n",
    "    BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "    pt = torch.exp(-BCE_loss)\n",
    "    F_loss = alpha * (1-pt)**gamma * BCE_loss\n",
    "    \n",
    "    if reduction == 'mean':\n",
    "        return torch.mean(F_loss)\n",
    "    elif reduction == 'sum':\n",
    "        return torch.sum(F_loss)\n",
    "    else:\n",
    "        return F_loss\n",
    "\n",
    "# Alpha = binary class weighting\n",
    "\n",
    "# Usage\n",
    "outputs = model(inputs)  # shape: [batch, classes, width, height]\n",
    "targets = ...  # shape: [batch, width, height]\n",
    "loss = focal_loss(outputs[:, 1], targets)  # Assuming binary classification, use the positive class logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palsa_env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
