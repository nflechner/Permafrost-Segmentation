{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "labelitos = pd.read_csv(\"/Users/nadja/Documents/1. Project/Thesis/Permafrost-Segmentation/Supervised_dataset/orig_palsa_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.155569256062474"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelitos['palsa_percentage'].sum()/len(labelitos['palsa_percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = SegformerImageProcessor(\n",
    "            image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "            image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "            do_reduce_labels=False\n",
    "            )\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f.endswith('.jpg')][:196] #TODO REMOVE the 128!\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "##############\n",
    "# Custom Loss\n",
    "##############\n",
    "\n",
    "def weighted_cross_entropy_loss(logits, targets, class_weights=[1, 24]):\n",
    "    \"\"\"\n",
    "    Calculate weighted cross-entropy loss for binary segmentation using PyTorch's built-in functions.\n",
    "    \n",
    "    Args:\n",
    "    logits (torch.Tensor): Predicted logits with shape [batch, num_classes, height, width]\n",
    "    targets (torch.Tensor): Ground truth labels with shape [batch, height, width]\n",
    "    class_weights (list): Weights for each class [weight_class_0, weight_class_1]\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Weighted cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    device = logits.device\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    # Convert class weights to a tensor and move to the same device\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Create the loss function with weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "    \n",
    "    # Calculate and return the loss\n",
    "    return criterion(logits, targets)\n",
    "\n",
    "# Example usage:\n",
    "# logits = torch.randn(32, 2, 512, 512)  # [batch, num_classes, height, width]\n",
    "# targets = torch.randint(0, 2, (32, 512, 512))  # [batch, height, width]\n",
    "# loss = weighted_cross_entropy_loss(logits, targets)\n",
    "\n",
    "\n",
    "###################\n",
    "# Generate Datasets\n",
    "###################\n",
    "\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SemanticSegmentationDataset(root_dir)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64)\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b0\", \n",
    "    num_labels=2\n",
    ") \n",
    "\n",
    "# Freeze encoder layers\n",
    "for param in model.segformer.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optionally, unfreeze the last few layers of the encoder\n",
    "# Adjust the number of unfrozen blocks as needed\n",
    "num_unfrozen_blocks = 2\n",
    "for i in range(len(model.segformer.encoder.block) - num_unfrozen_blocks, len(model.segformer.encoder.block)):\n",
    "    for param in model.segformer.encoder.block[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "epochs = 2\n",
    "lr = 0.01\n",
    "warmup_steps = 100  # Adjust this value as needed\n",
    "\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = weighted_cross_entropy_loss\n",
    "\n",
    "\n",
    "# define scheduler\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# run = wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"Finetune_segformer\",\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"epochs\": 20,\n",
    "#         \"lr\": lr,\n",
    "#         \"warmup_steps\": warmup_steps,\n",
    "#         \"patience\": patience\n",
    "#         }\n",
    "# )\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        # outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        # loss, logits = outputs.loss, outputs.logits\n",
    "        \n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "        # scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        # progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    model.eval()\n",
    "    jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Calculate Jaccard score\n",
    "            # Convert logits to binary segmentation mask\n",
    "            predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "            \n",
    "            # Upsample the predicted mask to match the label size\n",
    "            upsampled_predicted = F.interpolate(predicted.unsqueeze(1).float(), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            jaccard = jaccard_index(upsampled_predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2)\n",
    "            jaccard_scores.append(jaccard.item())\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for target class only, if not a only background image\n",
    "            if len(labels.unique()) > 1:\n",
    "                target_jaccard = jaccard_index(upsampled_predicted.squeeze(1), labels, task=\"multiclass\", num_classes=2, average=\"none\")[1]\n",
    "                target_jaccard_scores.append(target_jaccard.item())\n",
    "\n",
    "    avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    # wandb.log({\"jaccard\": avg_jaccard})\n",
    "    # wandb.log({\"target_jaccard\": avg_target_jaccard})\n",
    "    print(f\"Epoch {epoch}, Average Jaccard Score: {avg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    # # Early stopping check based on target Jaccard score\n",
    "    # if avg_jaccard > best_jaccard:\n",
    "    #     best_jaccard = avg_jaccard\n",
    "    #     epochs_no_improve = 0\n",
    "    #     # Save the best model\n",
    "    #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # else:\n",
    "    #     epochs_no_improve += 1\n",
    "    #     if epochs_no_improve == patience:\n",
    "    #         print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "    #         break\n",
    "    \n",
    "\n",
    "\n",
    "# artifact = wandb.Artifact('finetuned_segformer', type='model')\n",
    "# artifact.add_file('best_model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall info\n",
    "\n",
    "Folder structure will be: \n",
    "\n",
    "Dataset \n",
    "    |__RGB \n",
    "    |__HS \n",
    "    |__DEM \n",
    "    |__annotations \n",
    "    |__labels.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe\n",
    "- All of the below script has been written but not tested \n",
    "- to run it, several installs might be needed. \n",
    "\n",
    "##### Useful links: \n",
    "- [source code](https://github.com/huggingface/transformers/blob/main/src/transformers/models/segformer/image_processing_segformer.py)\n",
    "- https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, image_processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "    \n",
    "\"\"\"\n",
    "WARNING: \n",
    "by default the image processor below will resize the image (to 512*512).\n",
    "Essentially i don't want this, HOWEVER it might be necessary to be able\n",
    "to use the weights from pretraining. TODO: Find out whether that's so.\n",
    "\"\"\"\n",
    "\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "image_processor = SegformerImageProcessor(\n",
    "    image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "    image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "    do_reduce_labels=False\n",
    "    )\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SemanticSegmentationDataset(root_dir, image_processor)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "### IMPORTANT: DO I WANT TO FREEZE LAYERS??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b0\", \n",
    "    num_labels=1# since we treat '0' as a background, the only class is palsa.\n",
    ") \n",
    "\n",
    "# Freeze encoder layers\n",
    "for param in model.segformer.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optionally, unfreeze the last few layers of the encoder\n",
    "# Adjust the number of unfrozen blocks as needed\n",
    "num_unfrozen_blocks = 2\n",
    "for i in range(len(model.segformer.encoder.block) - num_unfrozen_blocks, len(model.segformer.encoder.block)):\n",
    "    for param in model.segformer.encoder.block[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# The decoder (model.decode_head) will be trained by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the size of next iteration of valid loader image and labels, as well as last model output size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune\n",
    "based on [huggingface tutorial](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb)\n",
    "\n",
    "DO i want to log all of this with wandb?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 194/194 [16:30<00:00,  5.11s/it, Loss=0.0190, LR=0.000059]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [128] and output size of torch.Size([512, 512]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Calculate Jaccard score\u001b[39;00m\n\u001b[1;32m     71\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# chooses class with highest probability per \u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m upsampled_logits \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Calculate overall Jaccard score (including background)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m jaccard \u001b[38;5;241m=\u001b[39m jaccard_index(upsampled_logits, labels, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/nn/functional.py:3983\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   3982\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m-> 3983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3984\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3985\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput with spatial dimensions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3986\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3987\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3988\u001b[0m         )\n\u001b[1;32m   3989\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m   3990\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_integer(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m size):\n",
      "\u001b[0;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [128] and output size of torch.Size([512, 512]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "\n",
    "epochs = 20\n",
    "lr = 0.00006\n",
    "warmup_steps = 100  # Adjust this value as needed\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# define scheduler\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    model.eval()\n",
    "    jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Calculate Jaccard score\n",
    "            # Since we only have one feature map, we can use a threshold to determine the segmentation\n",
    "            predicted = (logits.squeeze(1) > 0).float()  # Threshold at 0\n",
    "            upsampled_predicted = F.interpolate(predicted.unsqueeze(1), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            jaccard = jaccard_index(upsampled_predicted.squeeze(1), labels, task=\"multiclass\", num_classes=2)\n",
    "            jaccard_scores.append(jaccard.item())\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for target class only, if not a only background image\n",
    "            if len(labels.unique()) > 1:\n",
    "                target_jaccard = jaccard_index(upsampled_predicted.squeeze(1), labels, task=\"multiclass\", num_classes=2, average=\"none\")[1]\n",
    "                target_jaccard_scores.append(target_jaccard.item())\n",
    "\n",
    "    avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    print(f\"Epoch {epoch}, Average Jaccard Score: {avg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    # Early stopping check based on target Jaccard score\n",
    "    if avg_jaccard > best_jaccard:\n",
    "        best_jaccard = avg_jaccard\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "            break\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "# Load the best model after training\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a result with the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('imgpath')\n",
    "pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values=pixel_values)\n",
    "logits = outputs.logits.cpu()\n",
    "predicted_segmentation_map = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
    "predicted_segmentation_map = predicted_segmentation_map.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "color_seg = np.zeros((predicted_segmentation_map.shape[0],\n",
    "                      predicted_segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "\n",
    "color = np.array([4, 250, 7])\n",
    "color_seg[predicted_segmentation_map == 0, :] = color\n",
    "# Convert to BGR\n",
    "color_seg = color_seg[..., ::-1]\n",
    "\n",
    "# Show image + mask\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palsa_env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
