{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "labelitos = pd.read_csv(\"/Users/nadja/Documents/1. Project/Thesis/Permafrost-Segmentation/Supervised_dataset/orig_palsa_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.155569256062474"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelitos['palsa_percentage'].sum()/len(labelitos['palsa_percentage'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"/root/Permafrost-Segmentation/orig_palsa_labels.csv\", names=['filename', 'palsa'], header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = dataframe.loc[dataframe['palsa']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>palsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>760_73_5025_2018_crop_31</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>760_73_5025_2018_crop_56</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>760_73_5025_2018_crop_89</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>760_73_5025_2018_crop_123</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>760_73_5025_2018_negcrop_0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21883</th>\n",
       "      <td>763_72_2550_2018_negcrop_8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21884</th>\n",
       "      <td>763_72_2550_2018_negcrop_9</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21885</th>\n",
       "      <td>763_72_2550_2018_negcrop_10</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21887</th>\n",
       "      <td>763_72_2550_2018_crop_33_aug</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21892</th>\n",
       "      <td>763_72_2550_2018_crop_83_aug</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10267 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           filename  palsa\n",
       "11         760_73_5025_2018_crop_31    0.0\n",
       "24         760_73_5025_2018_crop_56    0.0\n",
       "37         760_73_5025_2018_crop_89    0.0\n",
       "44        760_73_5025_2018_crop_123    0.0\n",
       "47       760_73_5025_2018_negcrop_0    0.0\n",
       "...                             ...    ...\n",
       "21883    763_72_2550_2018_negcrop_8    0.0\n",
       "21884    763_72_2550_2018_negcrop_9    0.0\n",
       "21885   763_72_2550_2018_negcrop_10    0.0\n",
       "21887  763_72_2550_2018_crop_33_aug    0.0\n",
       "21892  763_72_2550_2018_crop_83_aug    0.0\n",
       "\n",
       "[10267 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21897"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 239/239 [06:55<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Jaccard Score: 0.0729, Target Class Jaccard Score: 0.1458\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 239/239 [08:56<00:00,  2.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Jaccard Score: 0.0729, Target Class Jaccard Score: 0.1458\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4:   1%|▏         | 3/239 [00:05<06:36,  1.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 185\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    184\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(train_dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:  \n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# get the inputs;\u001b[39;00m\n\u001b[1;32m    187\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    188\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx]] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/utils/data/dataset.py:420\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m, in \u001b[0;36mSemanticSegmentationDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     59\u001b[0m segmentation_map \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(ann_path)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# randomly crop + pad both image and segmentation map to same size\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m encoded_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegmentation_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m encoded_inputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     65\u001b[0m   encoded_inputs[k]\u001b[38;5;241m.\u001b[39msqueeze_() \u001b[38;5;66;03m# remove batch dimension\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:303\u001b[0m, in \u001b[0;36mSegformerImageProcessor.__call__\u001b[0;34m(self, images, segmentation_maps, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, segmentation_maps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    297\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m    Preprocesses a batch of images and optionally segmentation maps.\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;03m    Overrides the `__call__` method of the `Preprocessor` class so that both images and segmentation maps can be\u001b[39;00m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;03m    passed in as positional arguments.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegmentation_maps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegmentation_maps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/image_processing_utils.py:41\u001b[0m, in \u001b[0;36mBaseImageProcessor.__call__\u001b[0;34m(self, images, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchFeature:\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Preprocess an image or a batch of images.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/utils/deprecation.py:165\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/utils/generic.py:852\u001b[0m, in \u001b[0;36mfilter_out_non_signature_kwargs.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    843\u001b[0m         cls_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    845\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    846\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following named arguments are not valid for `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcls_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    847\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and were ignored: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minvalid_kwargs_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    848\u001b[0m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m    849\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    850\u001b[0m     )\n\u001b[0;32m--> 852\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalid_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:403\u001b[0m, in \u001b[0;36mSegformerImageProcessor.preprocess\u001b[0;34m(self, images, segmentation_maps, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, do_reduce_labels, return_tensors, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m     )\n\u001b[1;32m    392\u001b[0m validate_preprocess_arguments(\n\u001b[1;32m    393\u001b[0m     do_rescale\u001b[38;5;241m=\u001b[39mdo_rescale,\n\u001b[1;32m    394\u001b[0m     rescale_factor\u001b[38;5;241m=\u001b[39mrescale_factor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m    401\u001b[0m )\n\u001b[0;32m--> 403\u001b[0m images \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_image(\n\u001b[1;32m    405\u001b[0m         image\u001b[38;5;241m=\u001b[39mimg,\n\u001b[1;32m    406\u001b[0m         do_resize\u001b[38;5;241m=\u001b[39mdo_resize,\n\u001b[1;32m    407\u001b[0m         resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m    408\u001b[0m         size\u001b[38;5;241m=\u001b[39msize,\n\u001b[1;32m    409\u001b[0m         do_rescale\u001b[38;5;241m=\u001b[39mdo_rescale,\n\u001b[1;32m    410\u001b[0m         rescale_factor\u001b[38;5;241m=\u001b[39mrescale_factor,\n\u001b[1;32m    411\u001b[0m         do_normalize\u001b[38;5;241m=\u001b[39mdo_normalize,\n\u001b[1;32m    412\u001b[0m         image_mean\u001b[38;5;241m=\u001b[39mimage_mean,\n\u001b[1;32m    413\u001b[0m         image_std\u001b[38;5;241m=\u001b[39mimage_std,\n\u001b[1;32m    414\u001b[0m         data_format\u001b[38;5;241m=\u001b[39mdata_format,\n\u001b[1;32m    415\u001b[0m         input_data_format\u001b[38;5;241m=\u001b[39minput_data_format,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    418\u001b[0m ]\n\u001b[1;32m    420\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m segmentation_maps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:404\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    389\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    390\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor, tf.Tensor or jax.ndarray.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    391\u001b[0m     )\n\u001b[1;32m    392\u001b[0m validate_preprocess_arguments(\n\u001b[1;32m    393\u001b[0m     do_rescale\u001b[38;5;241m=\u001b[39mdo_rescale,\n\u001b[1;32m    394\u001b[0m     rescale_factor\u001b[38;5;241m=\u001b[39mrescale_factor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m    401\u001b[0m )\n\u001b[1;32m    403\u001b[0m images \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 404\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_resize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_resize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrescale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images\n\u001b[1;32m    418\u001b[0m ]\n\u001b[1;32m    420\u001b[0m data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m segmentation_maps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:243\u001b[0m, in \u001b[0;36mSegformerImageProcessor._preprocess_image\u001b[0;34m(self, image, do_resize, size, resample, do_rescale, rescale_factor, do_normalize, image_mean, image_std, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m input_data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    242\u001b[0m     input_data_format \u001b[38;5;241m=\u001b[39m infer_channel_dimension_format(image)\n\u001b[0;32m--> 243\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_reduce_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_resize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_resize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_rescale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_rescale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrescale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrescale_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_normalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_mean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_std\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_std\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     image \u001b[38;5;241m=\u001b[39m to_channel_dimension_format(image, data_format, input_channel_dim\u001b[38;5;241m=\u001b[39minput_data_format)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/models/segformer/image_processing_segformer.py:215\u001b[0m, in \u001b[0;36mSegformerImageProcessor._preprocess\u001b[0;34m(self, image, do_reduce_labels, do_resize, do_rescale, do_normalize, size, resample, rescale_factor, image_mean, image_std, input_data_format)\u001b[0m\n\u001b[1;32m    212\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrescale(image\u001b[38;5;241m=\u001b[39mimage, scale\u001b[38;5;241m=\u001b[39mrescale_factor, input_data_format\u001b[38;5;241m=\u001b[39minput_data_format)\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_normalize:\n\u001b[0;32m--> 215\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_std\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/image_processing_utils.py:111\u001b[0m, in \u001b[0;36mBaseImageProcessor.normalize\u001b[0;34m(self, image, mean, std, data_format, input_data_format, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnormalize\u001b[39m(\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     80\u001b[0m     image: np\u001b[38;5;241m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m     86\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m     87\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;124;03m    Normalize an image. image = (image - image_mean) / image_std.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m        `np.ndarray`: The normalized image.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_data_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_data_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/transformers/image_transforms.py:409\u001b[0m, in \u001b[0;36mnormalize\u001b[0;34m(image, mean, std, data_format, input_data_format)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    407\u001b[0m     image \u001b[38;5;241m=\u001b[39m ((image\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m-\u001b[39m mean) \u001b[38;5;241m/\u001b[39m std)\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 409\u001b[0m image \u001b[38;5;241m=\u001b[39m to_channel_dimension_format(image, data_format, input_data_format) \u001b[38;5;28;01mif\u001b[39;00m data_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m image\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = SegformerImageProcessor(\n",
    "            image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "            image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "            do_reduce_labels=False\n",
    "            )\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        dataframe = pd.read_csv(\n",
    "            f\"{root_dir}/orig_palsa_labels.csv\", \n",
    "            names=['filename', 'palsa'], \n",
    "            header=0\n",
    "            )\n",
    "        \n",
    "        dataframe = dataframe.loc[dataframe['palsa']>0]\n",
    "        dataframe = dataframe[~dataframe['filename'].str.endswith('aug')]\n",
    "        checked_names = list(dataframe['filename'])\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f[:-4] in checked_names]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "##############\n",
    "# Custom Loss\n",
    "##############\n",
    "\n",
    "def weighted_cross_entropy_loss(logits, targets, class_weights=[1, 24]):\n",
    "    \"\"\"\n",
    "    Calculate weighted cross-entropy loss for binary segmentation using PyTorch's built-in functions.\n",
    "    \n",
    "    Args:\n",
    "    logits (torch.Tensor): Predicted logits with shape [batch, num_classes, height, width]\n",
    "    targets (torch.Tensor): Ground truth labels with shape [batch, height, width]\n",
    "    class_weights (list): Weights for each class [weight_class_0, weight_class_1]\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Weighted cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    device = logits.device\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    # Convert class weights to a tensor and move to the same device\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Create the loss function with weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "    \n",
    "    # Calculate and return the loss\n",
    "    return criterion(logits, targets)\n",
    "\n",
    "# Example usage:\n",
    "# logits = torch.randn(32, 2, 512, 512)  # [batch, num_classes, height, width]\n",
    "# targets = torch.randint(0, 2, (32, 512, 512))  # [batch, height, width]\n",
    "# loss = weighted_cross_entropy_loss(logits, targets)\n",
    "\n",
    "\n",
    "###################\n",
    "# Generate Datasets\n",
    "###################\n",
    "\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SemanticSegmentationDataset(root_dir)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=20)#, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=20)\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b0\", \n",
    "    num_labels=2\n",
    ") \n",
    "\n",
    "# Freeze encoder layers\n",
    "for param in model.segformer.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optionally, unfreeze the last few layers of the encoder\n",
    "# Adjust the number of unfrozen blocks as needed\n",
    "num_unfrozen_blocks = 2\n",
    "for i in range(len(model.segformer.encoder.block) - num_unfrozen_blocks, len(model.segformer.encoder.block)):\n",
    "    for param in model.segformer.encoder.block[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "epochs = 4\n",
    "lr = 0.1\n",
    "warmup_steps = 100  # Adjust this value as needed\n",
    "\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "criterion = weighted_cross_entropy_loss\n",
    "\n",
    "\n",
    "# define scheduler\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# run = wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"Finetune_segformer\",\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"epochs\": epochs,\n",
    "#         \"lr\": lr,\n",
    "#         \"warmup_steps\": warmup_steps,\n",
    "#         \"patience\": patience\n",
    "#         }\n",
    "# )\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits.unsqueeze(1).float(), \n",
    "            size=[logits.shape[1],labels.shape[-2],labels.shape[-1]], \n",
    "            mode=\"nearest\")\n",
    "        loss = criterion(upsampled_logits.squeeze(1), labels)\n",
    "\n",
    "        # debugging\n",
    "        # predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "        # print(torch.argmax(logits, dim=1).sum())\n",
    "        # print(f\"loss = {loss}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        # progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    model.eval()\n",
    "    jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Calculate Jaccard score\n",
    "            # Convert logits to binary segmentation mask\n",
    "            predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "            \n",
    "            # Upsample the predicted mask to match the label size\n",
    "            upsampled_predicted = F.interpolate(predicted.unsqueeze(1).float(), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            jaccard = jaccard_index(upsampled_predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2)\n",
    "            jaccard_scores.append(jaccard)\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for target class only, if not a only background image\n",
    "            if len(labels.unique()) > 1:\n",
    "                target_jaccard = jaccard_index(upsampled_predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2, average=\"none\")[1]\n",
    "                target_jaccard_scores.append(target_jaccard)\n",
    "\n",
    "    avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    # wandb.log({\"jaccard\": avg_jaccard})\n",
    "    # wandb.log({\"target_jaccard\": avg_target_jaccard})\n",
    "    print(f\"Epoch {epoch}, Average Jaccard Score: {avg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    # # Early stopping check based on target Jaccard score\n",
    "    # if avg_jaccard > best_jaccard:\n",
    "    #     best_jaccard = avg_jaccard\n",
    "    #     epochs_no_improve = 0\n",
    "    #     # Save the best model\n",
    "    #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # else:\n",
    "    #     epochs_no_improve += 1\n",
    "    #     if epochs_no_improve == patience:\n",
    "    #         print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "    #         break\n",
    "    \n",
    "\n",
    "\n",
    "# artifact = wandb.Artifact('finetuned_segformer', type='model')\n",
    "# artifact.add_file('best_model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The run below worked surprisingly well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Average Jaccard Score: 0.4197, Target Class Jaccard Score: 0.0381\n",
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 75/75 [07:28<00:00,  5.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Jaccard Score: 0.2883, Target Class Jaccard Score: 0.1477\n",
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4: 100%|██████████| 75/75 [07:01<00:00,  5.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Average Jaccard Score: 0.1503, Target Class Jaccard Score: 0.1545\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/4: 100%|██████████| 75/75 [07:33<00:00,  6.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Average Jaccard Score: 0.2154, Target Class Jaccard Score: 0.1535\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/4: 100%|██████████| 75/75 [06:47<00:00,  5.44s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = SegformerImageProcessor(\n",
    "            image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "            image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "            do_reduce_labels=False\n",
    "            )\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        dataframe = pd.read_csv(\n",
    "            f\"{root_dir}/orig_palsa_labels.csv\", \n",
    "            names=['filename', 'palsa'], \n",
    "            header=0\n",
    "            )\n",
    "        \n",
    "        dataframe = dataframe.loc[dataframe['palsa']>0]\n",
    "        dataframe = dataframe[~dataframe['filename'].str.endswith('aug')]\n",
    "        checked_names = list(dataframe['filename'])\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f[:-4] in checked_names]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "##############\n",
    "# Custom Loss\n",
    "##############\n",
    "\n",
    "def weighted_cross_entropy_loss(logits, targets, class_weights=[1, 6]): # shuld be 1,24\n",
    "    \"\"\"\n",
    "    Calculate weighted cross-entropy loss for binary segmentation using PyTorch's built-in functions.\n",
    "    \n",
    "    Args:\n",
    "    logits (torch.Tensor): Predicted logits with shape [batch, num_classes, height, width]\n",
    "    targets (torch.Tensor): Ground truth labels with shape [batch, height, width]\n",
    "    class_weights (list): Weights for each class [weight_class_0, weight_class_1]\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Weighted cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    device = logits.device\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    # Convert class weights to a tensor and move to the same device\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Create the loss function with weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "    \n",
    "    # Calculate and return the loss\n",
    "    return criterion(logits, targets)\n",
    "\n",
    "# Example usage:\n",
    "# logits = torch.randn(32, 2, 512, 512)  # [batch, num_classes, height, width]\n",
    "# targets = torch.randint(0, 2, (32, 512, 512))  # [batch, height, width]\n",
    "# loss = weighted_cross_entropy_loss(logits, targets)\n",
    "\n",
    "\n",
    "###################\n",
    "# Generate Datasets\n",
    "###################\n",
    "\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SemanticSegmentationDataset(root_dir)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)#, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64)\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b0\", \n",
    "    num_labels=2\n",
    ") \n",
    "\n",
    "# Freeze encoder layers\n",
    "for param in model.segformer.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optionally, unfreeze the last few layers of the encoder\n",
    "# Adjust the number of unfrozen blocks as needed\n",
    "num_unfrozen_blocks = 4\n",
    "for i in range(len(model.segformer.encoder.block) - num_unfrozen_blocks, len(model.segformer.encoder.block)):\n",
    "    for param in model.segformer.encoder.block[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "\n",
    "epochs = 4\n",
    "lr = 0.0001\n",
    "warmup_steps = 100  # Adjust this value as needed\n",
    "\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay = 0.03)\n",
    "criterion = weighted_cross_entropy_loss\n",
    "\n",
    "\n",
    "# define scheduler\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# run = wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"Finetune_segformer\",\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"epochs\": epochs,\n",
    "#         \"lr\": lr,\n",
    "#         \"warmup_steps\": warmup_steps,\n",
    "#         \"patience\": patience\n",
    "#         }\n",
    "# )\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.eval()\n",
    "    jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Calculate Jaccard score\n",
    "            # Convert logits to binary segmentation mask\n",
    "            predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "            \n",
    "            # Upsample the predicted mask to match the label size\n",
    "            upsampled_predicted = F.interpolate(predicted.unsqueeze(1).float(), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            jaccard = jaccard_index(upsampled_predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2)\n",
    "            jaccard_scores.append(jaccard)\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for target class only, if not a only background image\n",
    "            if len(labels.unique()) > 1:\n",
    "                target_jaccard = jaccard_index(upsampled_predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2, average=\"none\")[1]\n",
    "                target_jaccard_scores.append(target_jaccard)\n",
    "\n",
    "    avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    # wandb.log({\"jaccard\": avg_jaccard})\n",
    "    # wandb.log({\"target_jaccard\": avg_target_jaccard})\n",
    "    print(f\"Epoch {epoch}, Average Jaccard Score: {avg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    model.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits.unsqueeze(1).float(), \n",
    "            size=[logits.shape[1],labels.shape[-2],labels.shape[-1]], \n",
    "            mode=\"nearest\")\n",
    "        loss = criterion(upsampled_logits.squeeze(1), labels)\n",
    "\n",
    "        # debugging\n",
    "        # predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "        # print(torch.argmax(logits, dim=1).sum())\n",
    "        # print(f\"loss = {loss}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        # progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    # # Early stopping check based on target Jaccard score\n",
    "    # if avg_jaccard > best_jaccard:\n",
    "    #     best_jaccard = avg_jaccard\n",
    "    #     epochs_no_improve = 0\n",
    "    #     # Save the best model\n",
    "    #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # else:\n",
    "    #     epochs_no_improve += 1\n",
    "    #     if epochs_no_improve == patience:\n",
    "    #         print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "    #         break\n",
    "    \n",
    "\n",
    "\n",
    "# artifact = wandb.Artifact('finetuned_segformer', type='model')\n",
    "# artifact.add_file('best_model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-class + background model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-ade-640-640 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([1, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|██████████| 5/5 [00:05<00:00,  1.01s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Detected the following values in `preds`: tensor([  0, 255], device='cuda:0') but expected only the following values [0,1] since preds is a label tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 191\u001b[0m\n\u001b[1;32m    186\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(upsampled_logits \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Calculate Jaccard score (IoU) for both classes\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# jaccard = jaccard_index(predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2)\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# or this?\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m jaccard \u001b[38;5;241m=\u001b[39m \u001b[43mjaccard_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m jaccard_scores\u001b[38;5;241m.\u001b[39mappend(jaccard)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# Calculate Jaccard score (IoU) for target class only, if not a only background image\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torchmetrics/functional/classification/jaccard.py:364\u001b[0m, in \u001b[0;36mjaccard_index\u001b[0;34m(preds, target, task, threshold, num_classes, num_labels, average, ignore_index, validate_args, zero_division)\u001b[0m\n\u001b[1;32m    362\u001b[0m task \u001b[38;5;241m=\u001b[39m ClassificationTask\u001b[38;5;241m.\u001b[39mfrom_str(task)\n\u001b[1;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m ClassificationTask\u001b[38;5;241m.\u001b[39mBINARY:\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_jaccard_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzero_division\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task \u001b[38;5;241m==\u001b[39m ClassificationTask\u001b[38;5;241m.\u001b[39mMULTICLASS:\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(num_classes, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torchmetrics/functional/classification/jaccard.py:154\u001b[0m, in \u001b[0;36mbinary_jaccard_index\u001b[0;34m(preds, target, threshold, ignore_index, validate_args, zero_division)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate_args:\n\u001b[1;32m    153\u001b[0m     _binary_confusion_matrix_arg_validation(threshold, ignore_index)\n\u001b[0;32m--> 154\u001b[0m     \u001b[43m_binary_confusion_matrix_tensor_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _binary_confusion_matrix_format(preds, target, threshold, ignore_index)\n\u001b[1;32m    156\u001b[0m confmat \u001b[38;5;241m=\u001b[39m _binary_confusion_matrix_update(preds, target)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torchmetrics/functional/classification/confusion_matrix.py:112\u001b[0m, in \u001b[0;36m_binary_confusion_matrix_tensor_validation\u001b[0;34m(preds, target, ignore_index)\u001b[0m\n\u001b[1;32m    110\u001b[0m unique_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39munique(preds)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39many((unique_values \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m&\u001b[39m (unique_values \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetected the following values in `preds`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but expected only\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the following values [0,1] since preds is a label tensor.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    115\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Detected the following values in `preds`: tensor([  0, 255], device='cuda:0') but expected only the following values [0,1] since preds is a label tensor."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = SegformerImageProcessor(\n",
    "            image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "            image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "            do_reduce_labels=True\n",
    "            )\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        dataframe = pd.read_csv(\n",
    "            f\"{root_dir}/orig_palsa_labels.csv\", \n",
    "            names=['filename', 'palsa'], \n",
    "            header=0\n",
    "            )\n",
    "        \n",
    "        dataframe = dataframe.loc[dataframe['palsa']>0]\n",
    "        dataframe = dataframe[~dataframe['filename'].str.endswith('aug')]\n",
    "        checked_names = list(dataframe['filename'])\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f[:-4] in checked_names][:30]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "\n",
    "###################\n",
    "# Generate Datasets\n",
    "###################\n",
    "\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SemanticSegmentationDataset(root_dir)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=5)#, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=5)\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b5-finetuned-ade-640-640\", \n",
    "    num_labels=1,\n",
    "    ignore_mismatched_sizes=True\n",
    ") \n",
    "\n",
    "# Freeze encoder layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "epochs = 4\n",
    "lr = 7e-5\n",
    "warmup_steps = 100  # Adjust this value as needed\n",
    "\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# define optimizer and loss\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay = 0.03)\n",
    "\n",
    "# define scheduler\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# run = wandb.init(\n",
    "#     # Set the project where this run will be logged\n",
    "#     project=\"Finetune_segformer\",\n",
    "#     # Track hyperparameters and run metadata\n",
    "#     config={\n",
    "#         \"epochs\": epochs,\n",
    "#         \"lr\": lr,\n",
    "#         \"warmup_steps\": warmup_steps,\n",
    "#         \"patience\": patience\n",
    "#         }\n",
    "# )\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    model.train()\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # debugging\n",
    "        # predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "        # print(torch.argmax(logits, dim=1).sum())\n",
    "        # print(f\"loss = {loss}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        # progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Upsample the predicted mask to match the label size\n",
    "            upsampled_logits = F.interpolate(logits.float(), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Convert logits to binary segmentation mask\n",
    "            predicted = torch.where(upsampled_logits > 0, 0, 255).float()\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            # jaccard = jaccard_index(predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2)\n",
    "            # or this?\n",
    "            jaccard = jaccard_index(predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2, ignore_index=255)\n",
    "            jaccard_scores.append(jaccard)\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for target class only, if not a only background image\n",
    "            if len(labels.unique()) > 1:\n",
    "                target_jaccard = jaccard_index(upsampled_predicted.squeeze(1).long(), labels, task=\"multiclass\", num_classes=2, average=\"none\")[1]\n",
    "                target_jaccard_scores.append(target_jaccard)\n",
    "\n",
    "    avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    # wandb.log({\"jaccard\": avg_jaccard})\n",
    "    # wandb.log({\"target_jaccard\": avg_target_jaccard})\n",
    "    print(f\"Epoch {epoch}, Average Jaccard Score: {avg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    # # Early stopping check based on target Jaccard score\n",
    "    # if avg_jaccard > best_jaccard:\n",
    "    #     best_jaccard = avg_jaccard\n",
    "    #     epochs_no_improve = 0\n",
    "    #     # Save the best model\n",
    "    #     torch.save(model.state_dict(), 'best_model.pth')\n",
    "    # else:\n",
    "    #     epochs_no_improve += 1\n",
    "    #     if epochs_no_improve == patience:\n",
    "    #         print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "    #         break\n",
    "    \n",
    "\n",
    "\n",
    "# artifact = wandb.Artifact('finetuned_segformer', type='model')\n",
    "# artifact.add_file('best_model.pth')\n",
    "# run.log_artifact(artifact)\n",
    "\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SegformerForSemanticSegmentation(\n",
       "  (segformer): SegformerModel(\n",
       "    (encoder): SegformerEncoder(\n",
       "      (patch_embeddings): ModuleList(\n",
       "        (0): SegformerOverlapPatchEmbeddings(\n",
       "          (proj): Conv2d(3, 32, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "          (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): SegformerOverlapPatchEmbeddings(\n",
       "          (proj): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): SegformerOverlapPatchEmbeddings(\n",
       "          (proj): Conv2d(64, 160, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): SegformerOverlapPatchEmbeddings(\n",
       "          (proj): Conv2d(160, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "          (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (block): ModuleList(\n",
       "        (0): ModuleList(\n",
       "          (0): SegformerLayer(\n",
       "            (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SegformerAttention(\n",
       "              (self): SegformerEfficientSelfAttention(\n",
       "                (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "                (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "                (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n",
       "                (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (output): SegformerSelfOutput(\n",
       "                (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (drop_path): Identity()\n",
       "            (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): SegformerMixFFN(\n",
       "              (dense1): Linear(in_features=32, out_features=128, bias=True)\n",
       "              (dwconv): SegformerDWConv(\n",
       "                (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (dense2): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SegformerLayer(\n",
       "            (layer_norm_1): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SegformerAttention(\n",
       "              (self): SegformerEfficientSelfAttention(\n",
       "                (query): Linear(in_features=32, out_features=32, bias=True)\n",
       "                (key): Linear(in_features=32, out_features=32, bias=True)\n",
       "                (value): Linear(in_features=32, out_features=32, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (sr): Conv2d(32, 32, kernel_size=(8, 8), stride=(8, 8))\n",
       "                (layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (output): SegformerSelfOutput(\n",
       "                (dense): Linear(in_features=32, out_features=32, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (drop_path): SegformerDropPath(p=0.014285714365541935)\n",
       "            (layer_norm_2): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): SegformerMixFFN(\n",
       "              (dense1): Linear(in_features=32, out_features=128, bias=True)\n",
       "              (dwconv): SegformerDWConv(\n",
       "                (dwconv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (dense2): Linear(in_features=128, out_features=32, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ModuleList(\n",
       "          (0): SegformerLayer(\n",
       "            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SegformerAttention(\n",
       "              (self): SegformerEfficientSelfAttention(\n",
       "                (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (output): SegformerSelfOutput(\n",
       "                (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (drop_path): SegformerDropPath(p=0.02857142873108387)\n",
       "            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): SegformerMixFFN(\n",
       "              (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (dwconv): SegformerDWConv(\n",
       "                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SegformerLayer(\n",
       "            (layer_norm_1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SegformerAttention(\n",
       "              (self): SegformerEfficientSelfAttention(\n",
       "                (query): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (key): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (value): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (sr): Conv2d(64, 64, kernel_size=(4, 4), stride=(4, 4))\n",
       "                (layer_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (output): SegformerSelfOutput(\n",
       "                (dense): Linear(in_features=64, out_features=64, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (drop_path): SegformerDropPath(p=0.04285714402794838)\n",
       "            (layer_norm_2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): SegformerMixFFN(\n",
       "              (dense1): Linear(in_features=64, out_features=256, bias=True)\n",
       "              (dwconv): SegformerDWConv(\n",
       "                (dwconv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (dense2): Linear(in_features=256, out_features=64, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ModuleList(\n",
       "          (0): SegformerLayer(\n",
       "            (layer_norm_1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SegformerAttention(\n",
       "              (self): SegformerEfficientSelfAttention(\n",
       "                (query): Linear(in_features=160, out_features=160, bias=True)\n",
       "                (key): Linear(in_features=160, out_features=160, bias=True)\n",
       "                (value): Linear(in_features=160, out_features=160, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n",
       "                (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (output): SegformerSelfOutput(\n",
       "                (dense): Linear(in_features=160, out_features=160, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (drop_path): SegformerDropPath(p=0.05714285746216774)\n",
       "            (layer_norm_2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): SegformerMixFFN(\n",
       "              (dense1): Linear(in_features=160, out_features=640, bias=True)\n",
       "              (dwconv): SegformerDWConv(\n",
       "                (dwconv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (dense2): Linear(in_features=640, out_features=160, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SegformerLayer(\n",
       "            (layer_norm_1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SegformerAttention(\n",
       "              (self): SegformerEfficientSelfAttention(\n",
       "                (query): Linear(in_features=160, out_features=160, bias=True)\n",
       "                (key): Linear(in_features=160, out_features=160, bias=True)\n",
       "                (value): Linear(in_features=160, out_features=160, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "                (sr): Conv2d(160, 160, kernel_size=(2, 2), stride=(2, 2))\n",
       "                (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "              (output): SegformerSelfOutput(\n",
       "                (dense): Linear(in_features=160, out_features=160, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (drop_path): SegformerDropPath(p=0.0714285746216774)\n",
       "            (layer_norm_2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): SegformerMixFFN(\n",
       "              (dense1): Linear(in_features=160, out_features=640, bias=True)\n",
       "              (dwconv): SegformerDWConv(\n",
       "                (dwconv): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=640)\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (dense2): Linear(in_features=640, out_features=160, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (3): ModuleList(\n",
       "          (0): SegformerLayer(\n",
       "            (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SegformerAttention(\n",
       "              (self): SegformerEfficientSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): SegformerSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (drop_path): SegformerDropPath(p=0.08571428805589676)\n",
       "            (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): SegformerMixFFN(\n",
       "              (dense1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (dwconv): SegformerDWConv(\n",
       "                (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (1): SegformerLayer(\n",
       "            (layer_norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (attention): SegformerAttention(\n",
       "              (self): SegformerEfficientSelfAttention(\n",
       "                (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (output): SegformerSelfOutput(\n",
       "                (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (drop_path): SegformerDropPath(p=0.10000000149011612)\n",
       "            (layer_norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): SegformerMixFFN(\n",
       "              (dense1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (dwconv): SegformerDWConv(\n",
       "                (dwconv): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "              )\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "              (dense2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): ModuleList(\n",
       "        (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "        (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)\n",
       "        (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decode_head): SegformerDecodeHead(\n",
       "    (linear_c): ModuleList(\n",
       "      (0): SegformerMLP(\n",
       "        (proj): Linear(in_features=32, out_features=256, bias=True)\n",
       "      )\n",
       "      (1): SegformerMLP(\n",
       "        (proj): Linear(in_features=64, out_features=256, bias=True)\n",
       "      )\n",
       "      (2): SegformerMLP(\n",
       "        (proj): Linear(in_features=160, out_features=256, bias=True)\n",
       "      )\n",
       "      (3): SegformerMLP(\n",
       "        (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "      )\n",
       "    )\n",
       "    (linear_fuse): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (batch_norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (activation): ReLU()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (classifier): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall info\n",
    "\n",
    "Folder structure will be: \n",
    "\n",
    "Dataset \n",
    "    |__RGB \n",
    "    |__HS \n",
    "    |__DEM \n",
    "    |__annotations \n",
    "    |__labels.csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe\n",
    "- All of the below script has been written but not tested \n",
    "- to run it, several installs might be needed. \n",
    "\n",
    "##### Useful links: \n",
    "- [source code](https://github.com/huggingface/transformers/blob/main/src/transformers/models/segformer/image_processing_segformer.py)\n",
    "- https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, image_processor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = image_processor\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f.endswith('.jpg')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "    \n",
    "\"\"\"\n",
    "WARNING: \n",
    "by default the image processor below will resize the image (to 512*512).\n",
    "Essentially i don't want this, HOWEVER it might be necessary to be able\n",
    "to use the weights from pretraining. TODO: Find out whether that's so.\n",
    "\"\"\"\n",
    "\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "image_processor = SegformerImageProcessor(\n",
    "    image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "    image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "    do_reduce_labels=False\n",
    "    )\n",
    "\n",
    "# Create the full dataset\n",
    "full_dataset = SemanticSegmentationDataset(root_dir, image_processor)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model\n",
    "\n",
    "### IMPORTANT: DO I WANT TO FREEZE LAYERS??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b0 and are newly initialized: ['decode_head.batch_norm.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_mean', 'decode_head.batch_norm.running_var', 'decode_head.batch_norm.weight', 'decode_head.classifier.bias', 'decode_head.classifier.weight', 'decode_head.linear_c.0.proj.bias', 'decode_head.linear_c.0.proj.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.2.proj.weight', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "# define model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b0\", \n",
    "    num_labels=1# since we treat '0' as a background, the only class is palsa.\n",
    ") \n",
    "\n",
    "# Freeze encoder layers\n",
    "for param in model.segformer.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Optionally, unfreeze the last few layers of the encoder\n",
    "# Adjust the number of unfrozen blocks as needed\n",
    "num_unfrozen_blocks = 2\n",
    "for i in range(len(model.segformer.encoder.block) - num_unfrozen_blocks, len(model.segformer.encoder.block)):\n",
    "    for param in model.segformer.encoder.block[i].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "# The decoder (model.decode_head) will be trained by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check the size of next iteration of valid loader image and labels, as well as last model output size. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune\n",
    "based on [huggingface tutorial](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb)\n",
    "\n",
    "DO i want to log all of this with wandb?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 194/194 [16:30<00:00,  5.11s/it, Loss=0.0190, LR=0.000059]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [128] and output size of torch.Size([512, 512]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Calculate Jaccard score\u001b[39;00m\n\u001b[1;32m     71\u001b[0m predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# chooses class with highest probability per \u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m upsampled_logits \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbilinear\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Calculate overall Jaccard score (including background)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m jaccard \u001b[38;5;241m=\u001b[39m jaccard_index(upsampled_logits, labels, task\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.10/site-packages/torch/nn/functional.py:3983\u001b[0m, in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[1;32m   3981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m   3982\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[0;32m-> 3983\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3984\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3985\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput with spatial dimensions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3986\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3987\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3988\u001b[0m         )\n\u001b[1;32m   3989\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n\u001b[1;32m   3990\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(_is_integer(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m size):\n",
      "\u001b[0;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [128] and output size of torch.Size([512, 512]). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "\n",
    "epochs = 20\n",
    "lr = 0.00006\n",
    "warmup_steps = 100  # Adjust this value as needed\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "# define scheduler\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Move optimizer to GPU (possibly unneccessary)\n",
    "for state in optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.to(device)\n",
    "\n",
    "# Early stopping parameters\n",
    "patience = 5\n",
    "best_jaccard = 0\n",
    "epochs_no_improve = 0\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch: {epoch}\")\n",
    "    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch in progress_bar:  \n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\"Loss\": f\"{loss.item():.4f}\", \"LR\": f\"{scheduler.get_last_lr()[0]:.6f}\"})\n",
    "\n",
    "    model.eval()\n",
    "    jaccard_scores = []\n",
    "    target_jaccard_scores = []\n",
    "    with torch.no_grad():\n",
    "        for batch in valid_dataloader:\n",
    "            # get the inputs;\n",
    "            pixel_values = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "            # Calculate Jaccard score\n",
    "            # Since we only have one feature map, we can use a threshold to determine the segmentation\n",
    "            predicted = (logits.squeeze(1) > 0).float()  # Threshold at 0\n",
    "            upsampled_predicted = F.interpolate(predicted.unsqueeze(1), size=labels.shape[-2:], mode=\"nearest\")\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for both classes\n",
    "            jaccard = jaccard_index(upsampled_predicted.squeeze(1), labels, task=\"multiclass\", num_classes=2)\n",
    "            jaccard_scores.append(jaccard.item())\n",
    "\n",
    "            # Calculate Jaccard score (IoU) for target class only, if not a only background image\n",
    "            if len(labels.unique()) > 1:\n",
    "                target_jaccard = jaccard_index(upsampled_predicted.squeeze(1), labels, task=\"multiclass\", num_classes=2, average=\"none\")[1]\n",
    "                target_jaccard_scores.append(target_jaccard.item())\n",
    "\n",
    "    avg_jaccard = sum(jaccard_scores) / len(jaccard_scores)\n",
    "    avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "    print(f\"Epoch {epoch}, Average Jaccard Score: {avg_jaccard:.4f}, Target Class Jaccard Score: {avg_target_jaccard:.4f}\")\n",
    "    \n",
    "    # Early stopping check based on target Jaccard score\n",
    "    if avg_jaccard > best_jaccard:\n",
    "        best_jaccard = avg_jaccard\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience:\n",
    "            print(f\"Early stopping triggered. No improvement in target Jaccard score for {patience} epochs.\")\n",
    "            break\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "# Load the best model after training\n",
    "model.load_state_dict(torch.load('best_model.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize a result with the trained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('imgpath')\n",
    "pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values=pixel_values)\n",
    "logits = outputs.logits.cpu()\n",
    "predicted_segmentation_map = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n",
    "predicted_segmentation_map = predicted_segmentation_map.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "color_seg = np.zeros((predicted_segmentation_map.shape[0],\n",
    "                      predicted_segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n",
    "\n",
    "color = np.array([4, 250, 7])\n",
    "color_seg[predicted_segmentation_map == 0, :] = color\n",
    "# Convert to BGR\n",
    "color_seg = color_seg[..., ::-1]\n",
    "\n",
    "# Show image + mask\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palsa_env_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
