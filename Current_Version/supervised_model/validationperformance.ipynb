{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact finetuned_segformer:v208, 323.17MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.9\n",
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b5-finetuned-ade-640-640 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 768, 1, 1]) in the checkpoint and torch.Size([2, 768, 1, 1]) in the model instantiated\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_2859/370606458.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f\"{artifact_dir}/best_model.pth\", map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from torchmetrics.functional.classification import multiclass_accuracy\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = SegformerImageProcessor(\n",
    "            image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "            image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "            do_reduce_labels=False\n",
    "            )\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"masks\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        dataframe = pd.read_csv(\n",
    "            f\"{root_dir}/orig_palsa_labels.csv\", \n",
    "            names=['filename', 'palsa'], \n",
    "            header=0\n",
    "            )\n",
    "        \n",
    "        dataframe = dataframe.loc[dataframe['palsa']>0]\n",
    "        dataframe = dataframe[~dataframe['filename'].str.endswith('aug')]\n",
    "        checked_names = list(dataframe['filename'])\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f[:-4] in checked_names]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "##############\n",
    "# Custom Loss\n",
    "##############\n",
    "\n",
    "def weighted_cross_entropy_loss(logits, targets, class_weights=[1, 6]): # shuld be 1,24\n",
    "    \"\"\"\n",
    "    Calculate weighted cross-entropy loss for binary segmentation using PyTorch's built-in functions.\n",
    "    \n",
    "    Args:\n",
    "    logits (torch.Tensor): Predicted logits with shape [batch, num_classes, height, width]\n",
    "    targets (torch.Tensor): Ground truth labels with shape [batch, height, width]\n",
    "    class_weights (list): Weights for each class [weight_class_0, weight_class_1]\n",
    "    \n",
    "    Returns:\n",
    "    torch.Tensor: Weighted cross-entropy loss\n",
    "    \"\"\"\n",
    "    # Ensure inputs are on the same device\n",
    "    device = logits.device\n",
    "    targets = targets.to(device)\n",
    "    \n",
    "    # Convert class weights to a tensor and move to the same device\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Create the loss function with weights\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, reduction='mean')\n",
    "    \n",
    "    # Calculate and return the loss\n",
    "    return criterion(logits, targets)\n",
    "\n",
    "# Example usage:\n",
    "# logits = torch.randn(32, 2, 512, 512)  # [batch, num_classes, height, width]\n",
    "# targets = torch.randint(0, 2, (32, 512, 512))  # [batch, height, width]\n",
    "# loss = weighted_cross_entropy_loss(logits, targets)\n",
    "\n",
    "#########\n",
    "# CONFIGS\n",
    "#########\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 5\n",
    "\n",
    "\n",
    "# Create the full dataset\n",
    "root_dir = \"/root/Permafrost-Segmentation/Supervised_dataset\"\n",
    "# root_dir = \"/home/nadjaflechner/Permafrost-Segmentation/Supervised_dataset\"\n",
    "full_dataset = SemanticSegmentationDataset(root_dir)\n",
    "\n",
    "# Split the dataset into 85% train and 15% validation\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.85 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(full_dataset, [train_size, valid_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)#, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "model_name = \"nvidia/segformer-b5-finetuned-ade-640-640\"\n",
    "# model_name = \"sawthiha/segformer-b0-finetuned-deprem-satellite\"\n",
    "\n",
    "# Initialize wandb API\n",
    "api = wandb.Api()\n",
    "\n",
    "# Specify the artifact path\n",
    "artifact_name = 'finetuned_segformer:v208'\n",
    "artifact_path = f'nadjaflechner/Finetune_segformer_sweep/{artifact_name}'\n",
    "\n",
    "# Download the artifact\n",
    "artifact = api.artifact(artifact_path)\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "# Load the model\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=2,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(device)\n",
    "\n",
    "# Load the state dict\n",
    "state_dict = torch.load(f\"{artifact_dir}/best_model.pth\", map_location=device)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "bg_jaccard_scores = []\n",
    "target_jaccard_scores = []\n",
    "overall_accuracy = []\n",
    "bg_accuracy = []\n",
    "target_accuracy = []\n",
    "\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    for batch in train_dataloader:\n",
    "        print(counter)\n",
    "        counter +=1\n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits.unsqueeze(1).float(), \n",
    "            size=[logits.shape[1],labels.shape[-2],labels.shape[-1]], \n",
    "            mode=\"nearest\")\n",
    "\n",
    "        # Convert logits to binary segmentation mask\n",
    "        predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "        \n",
    "        # Upsample the predicted mask to match the label size\n",
    "        upsampled_predicted = F.interpolate(\n",
    "            predicted.unsqueeze(1).float(), \n",
    "            size=labels.shape[-2:], \n",
    "            mode=\"nearest\"\n",
    "        )\n",
    "\n",
    "        # Calculate Jaccard score (IoU) for both classes\n",
    "        jaccard = jaccard_index(\n",
    "            upsampled_predicted.squeeze(1).long(), \n",
    "            labels, \n",
    "            task=\"multiclass\", \n",
    "            num_classes=2, \n",
    "            average='none'\n",
    "        )\n",
    "        bg_jaccard_scores.append(jaccard[0])\n",
    "        target_jaccard_scores.append(jaccard[1])\n",
    "\n",
    "        # Overall accuracy\n",
    "        accuracy = multiclass_accuracy(\n",
    "            upsampled_predicted.squeeze(1).long(), \n",
    "            labels, \n",
    "            num_classes=2, \n",
    "            average='micro'\n",
    "        )\n",
    "        overall_accuracy.append(accuracy)\n",
    "\n",
    "        # Overall accuracy\n",
    "        accuracy = multiclass_accuracy(\n",
    "            upsampled_predicted.squeeze(1).long(), \n",
    "            labels, \n",
    "            num_classes=2, \n",
    "            average='none'\n",
    "        )\n",
    "        bg_accuracy.append(accuracy[0])\n",
    "        target_accuracy.append(accuracy[1])\n",
    "\n",
    "avg_bg_jaccard = sum(bg_jaccard_scores) / len(bg_jaccard_scores)\n",
    "avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "avg_overall_accuracy = sum(overall_accuracy) / len(overall_accuracy)\n",
    "avg_bg_accuracy = sum(bg_accuracy) / len(bg_accuracy)\n",
    "avg_target_accuracy = sum(target_accuracy) / len(target_accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_bg_jaccard = 0.9735082983970642\n",
      "avg_target_jaccard = 0.8421018719673157\n",
      "avg_overall_accuracy = 0.9779070019721985\n",
      "avg_bg_accuracy = 0.9874820113182068\n",
      "avg_target_accuracy = 0.9063408970832825\n"
     ]
    }
   ],
   "source": [
    "print(f\"avg_bg_jaccard = {avg_bg_jaccard}\") \n",
    "print(f\"avg_target_jaccard = {avg_target_jaccard}\") \n",
    "print(f\"avg_overall_accuracy = {avg_overall_accuracy}\") \n",
    "print(f\"avg_bg_accuracy = {avg_bg_accuracy}\") \n",
    "print(f\"avg_target_accuracy = {avg_target_accuracy}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_bg_jaccard = 0.9736375212669373\n",
      "avg_target_jaccard = 0.84956955909729\n",
      "avg_overall_accuracy = 0.9780684113502502\n",
      "avg_bg_accuracy = 0.9868788123130798\n",
      "avg_target_accuracy = 0.9128172397613525\n"
     ]
    }
   ],
   "source": [
    "print(f\"avg_bg_jaccard = {avg_bg_jaccard}\") \n",
    "print(f\"avg_target_jaccard = {avg_target_jaccard}\") \n",
    "print(f\"avg_overall_accuracy = {avg_overall_accuracy}\") \n",
    "print(f\"avg_bg_accuracy = {avg_bg_accuracy}\") \n",
    "print(f\"avg_target_accuracy = {avg_target_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from torchmetrics.functional.classification import multiclass_accuracy\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.nn.functional as F\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "from transformers import SegformerImageProcessor\n",
    "import pandas as pd \n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "# adapted from https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SegFormer/Fine_tune_SegFormer_on_custom_dataset.ipynb\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    \"\"\"Image (semantic) segmentation dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Root directory of the dataset containing the images + annotations.\n",
    "            image_processor (SegformerImageProcessor): image processor to prepare images + segmentation maps.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_processor = SegformerImageProcessor(\n",
    "            image_mean = [74.90, 85.26, 80.06], # use mean calculated over our dataset\n",
    "            image_std = [15.05, 13.88, 12.01], # use std calculated over our dataset\n",
    "            do_reduce_labels=False\n",
    "            )\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"jpg_rgb\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"png_GT\")\n",
    "        \n",
    "        # Get all image filenames without extension\n",
    "        dataframe = pd.read_csv(\n",
    "            f\"{root_dir}/new_palsa_labels.csv\", \n",
    "            names=['filename', 'palsa', 'matthias', 'difference'], \n",
    "            header=0\n",
    "            )\n",
    "        \n",
    "        dataframe = dataframe.loc[dataframe['palsa']>0]\n",
    "        checked_names = list(dataframe['filename'])\n",
    "        self.filenames = [os.path.splitext(f)[0] for f in os.listdir(self.img_dir) if f[:-4] in checked_names]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.filenames[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_name}.jpg\")\n",
    "        ann_path = os.path.join(self.ann_dir, f\"{img_name}.png\")\n",
    "\n",
    "        image = Image.open(img_path)\n",
    "        segmentation_map = Image.open(ann_path)\n",
    "\n",
    "        # randomly crop + pad both image and segmentation map to same size\n",
    "        encoded_inputs = self.image_processor(image, segmentation_map, return_tensors=\"pt\")\n",
    "\n",
    "        for k,v in encoded_inputs.items():\n",
    "          encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "\n",
    "        return encoded_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset\n",
    "data_directory = \"/root/Permafrost-Segmentation/Verified_GT/\"\n",
    "full_dataset = SemanticSegmentationDataset(data_directory)\n",
    "test_loader = DataLoader(full_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bg_jaccard_scores = []\n",
    "target_jaccard_scores = []\n",
    "overall_accuracy = []\n",
    "bg_accuracy = []\n",
    "target_accuracy = []\n",
    "\n",
    "counter = 0\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        print(counter)\n",
    "        counter +=1\n",
    "        # get the inputs;\n",
    "        pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        logits = outputs.logits\n",
    "        upsampled_logits = F.interpolate(\n",
    "            logits.unsqueeze(1).float(), \n",
    "            size=[logits.shape[1],labels.shape[-2],labels.shape[-1]], \n",
    "            mode=\"nearest\")\n",
    "\n",
    "        # Convert logits to binary segmentation mask\n",
    "        predicted = torch.argmax(logits, dim=1)  # Shape: (batch_size, 128, 128)\n",
    "        \n",
    "        # Upsample the predicted mask to match the label size\n",
    "        upsampled_predicted = F.interpolate(\n",
    "            predicted.unsqueeze(1).float(), \n",
    "            size=labels.shape[-2:], \n",
    "            mode=\"nearest\"\n",
    "        )\n",
    "\n",
    "        # Calculate Jaccard score (IoU) for both classes\n",
    "        jaccard = jaccard_index(\n",
    "            upsampled_predicted.squeeze(1).long(), \n",
    "            labels, \n",
    "            task=\"multiclass\", \n",
    "            num_classes=2, \n",
    "            average='none'\n",
    "        )\n",
    "        bg_jaccard_scores.append(jaccard[0])\n",
    "        target_jaccard_scores.append(jaccard[1])\n",
    "\n",
    "        # Overall accuracy\n",
    "        accuracy = multiclass_accuracy(\n",
    "            upsampled_predicted.squeeze(1).long(), \n",
    "            labels, \n",
    "            num_classes=2, \n",
    "            average='micro'\n",
    "        )\n",
    "        overall_accuracy.append(accuracy)\n",
    "\n",
    "        # Overall accuracy\n",
    "        accuracy = multiclass_accuracy(\n",
    "            upsampled_predicted.squeeze(1).long(), \n",
    "            labels, \n",
    "            num_classes=2, \n",
    "            average='none'\n",
    "        )\n",
    "        bg_accuracy.append(accuracy[0])\n",
    "        target_accuracy.append(accuracy[1])\n",
    "\n",
    "avg_bg_jaccard = sum(bg_jaccard_scores) / len(bg_jaccard_scores)\n",
    "avg_target_jaccard = sum(target_jaccard_scores) / len(target_jaccard_scores)\n",
    "avg_overall_accuracy = sum(overall_accuracy) / len(overall_accuracy)\n",
    "avg_bg_accuracy = sum(bg_accuracy) / len(bg_accuracy)\n",
    "avg_target_accuracy = sum(target_accuracy) / len(target_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_bg_jaccard = 0.8972979187965393\n",
      "avg_target_jaccard = 0.37508147954940796\n",
      "avg_overall_accuracy = 0.9080604314804077\n",
      "avg_bg_accuracy = 0.9302202463150024\n",
      "avg_target_accuracy = 0.6624864935874939\n"
     ]
    }
   ],
   "source": [
    "print(f\"avg_bg_jaccard = {avg_bg_jaccard}\") \n",
    "print(f\"avg_target_jaccard = {avg_target_jaccard}\") \n",
    "print(f\"avg_overall_accuracy = {avg_overall_accuracy}\") \n",
    "print(f\"avg_bg_accuracy = {avg_bg_accuracy}\") \n",
    "print(f\"avg_target_accuracy = {avg_target_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
