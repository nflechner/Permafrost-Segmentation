{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Imports #\n",
    "############\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from model.cnn_classifier import model_4D\n",
    "\n",
    "from model.pseudomask import Pseudomasks\n",
    "from utils.data_modules import ImageDataset, TestSet, filter_dataset\n",
    "\n",
    "##################\n",
    "## load configs ##\n",
    "##################\n",
    "\n",
    "# use this path when using vs code debugger.\n",
    "# config_path = os.path.join('/home/nadjaflechner/palsa_seg/current_models/pseudomask_generation_model', 'configs.json')\n",
    "\n",
    "config_path = os.path.join(os.getcwd(), 'configs/pseudomasks_configs.json')\n",
    "with open(config_path, 'r') as config_file:\n",
    "    configs = json.load(config_file)\n",
    "\n",
    "# load paths configs dictionary\n",
    "config_paths = configs.get('paths', {})\n",
    "# assign paths\n",
    "palsa_shapefile = config_paths.get('palsa_shapefile')\n",
    "final_pseudomasks_dir = config_paths.get('final_pseudomasks_dir')\n",
    "testset_dir = config_paths.get('testset')\n",
    "parent_dir = config_paths.get('data')\n",
    "rgb_dir = os.path.join(parent_dir, 'rgb')\n",
    "hs_dir = os.path.join(parent_dir, 'hs')\n",
    "dem_dir = os.path.join(parent_dir, 'dem')\n",
    "labels_file = os.path.join(parent_dir, 'palsa_labels.csv')\n",
    "\n",
    "# load model configs dictionary\n",
    "config_model = configs.get('model', {})\n",
    "# assign model\n",
    "artifact_path = config_model.get('artifact_path')\n",
    "run_id = config_model.get('run_id')\n",
    "finetune = config_model.get('finetuned')\n",
    "\n",
    "# load data configs dictionary\n",
    "config_data = configs.get('data', {})\n",
    "# assign data configs\n",
    "n_samples = config_data.get('n_samples')\n",
    "batch_size = config_data.get('batch_size')\n",
    "im_size = config_data.get('im_size')\n",
    "min_palsa_positive_samples = config_data.get('min_palsa_positive_samples')\n",
    "augment = config_data.get('augment')\n",
    "normalize = config_data.get('normalize')\n",
    "depth_layer = config_data.get('depth_layer')\n",
    "\n",
    "# load pseudomasks configs dictionary\n",
    "config_pseudomasks = configs.get('pseudomasks', {})\n",
    "# assign pseudomasks configs\n",
    "cam_threshold_factor = config_pseudomasks.get('cam_threshold_factor')\n",
    "overlap_threshold = config_pseudomasks.get('overlap_threshold')\n",
    "snic_seeds = config_pseudomasks.get('snic_seeds')\n",
    "snic_compactness = config_pseudomasks.get('snic_compactness')\n",
    "\n",
    "\n",
    "#########################\n",
    "# configure dataloaders #\n",
    "#########################\n",
    "\n",
    "train_files, val_files = filter_dataset(\n",
    "    labels_file = labels_file,\n",
    "    augment = augment, ### DO I WANT AUGMENTED IMGS TO HAVE PSEUDOMASK? \n",
    "    min_palsa_positive_samples = min_palsa_positive_samples,\n",
    "    low_pals_in_val = False, \n",
    "    n_samples = n_samples\n",
    "    )\n",
    "all_files = pd.concat([train_files, val_files])\n",
    "\n",
    "# choose depth data based on configs\n",
    "depth_dir = hs_dir if depth_layer == \"hs\" else dem_dir\n",
    "# Create the dataset and loaders for the entire dataset.\n",
    "dataset = ImageDataset(depth_dir, rgb_dir, all_files, im_size, normalize)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "#############\n",
    "# Init model: \n",
    "\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact(artifact_path, type='model')\n",
    "artifact_dir = artifact.download()\n",
    "state_dict = torch.load(f\"{artifact_dir}/model.pth\")\n",
    "model = model_4D()\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "#############################\n",
    "# generate all pseudolabels #\n",
    "#############################\n",
    "\n",
    "pseudomask_generator = Pseudomasks(\n",
    "    dataset, cam_threshold_factor, overlap_threshold,\n",
    "    snic_seeds, snic_compactness, finetuned = finetune\n",
    "    )\n",
    "pseudomask_generator.model = model\n",
    "\n",
    "for im,binary_label,_,img_name in loader:\n",
    "    if binary_label == 0: \n",
    "        pseudomask = np.full((400, 400), False, dtype=bool)\n",
    "    else:\n",
    "        pseudomask = pseudomask_generator.generate_mask(im, None, save_plot=False)\n",
    "    # Update the metadata for the cropped TIF\n",
    "    cropped_meta = im.meta.copy()\n",
    "    cropped_meta.update({\"driver\": \"GTiff\",\n",
    "                        \"height\": im.shape[1],\n",
    "                        \"width\": im.shape[2]})\n",
    "    \n",
    "    print(type(pseudomask))\n",
    "    print(pseudomask.shape)\n",
    "\n",
    "    break\n",
    "    # Save the cropped TIF file with a unique name\n",
    "    output_path = os.path.join(final_pseudomasks_dir, f\"{img_name}.tif\") # with original image name i can retrieve palsa % again. \n",
    "    with rasterio.open(output_path, \"w\", **cropped_meta) as dest:\n",
    "        dest.write(pseudomask)\n",
    "\n",
    "\n",
    "##############\n",
    "# finish run #\n",
    "##############\n",
    "\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
