{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To do list for implementation from scratch\n",
    "\n",
    "*Resources*\n",
    "\n",
    "* https://brsoff.github.io/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
    "* https://rumn.medium.com/part-1-ultimate-guide-to-fine-tuning-in-pytorch-pre-trained-model-and-its-configuration-8990194b71e\n",
    "* https://pytorch.org/vision/stable/models/generated/torchvision.models.vgg16.html#torchvision.models.vgg16\n",
    "\n",
    "*Stappenplan*\n",
    "* Look at details of how SMILIES implemented VGG.\n",
    "* Prepare and transform data (according to needs of pretrained model)\n",
    "* Look into what pytorch calls 'feature extraction' (only changing classification head), not 'fine tuning' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <E03EDA44-89AE-3115-9796-62BA9E0E2EDE> /Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <F75BB06B-2723-344D-99CE-9CB8BB94077A> /Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained VGG \"Accepts PIL.Image, batched (B, C, H, W) and single (C, H, W) image torch.Tensor objects\"\n",
    "\n",
    "* must convert the TIF into a format that it can be read by PIL, or use single Tensor objects\n",
    "* But if using batch i assume PIL must be used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = labels_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.index[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{img_name}.tif\")\n",
    "\n",
    "        # Open the TIF image using rasterio\n",
    "        with rasterio.open(img_path) as src:\n",
    "            # Read the image data\n",
    "            image_data = src.read()\n",
    "        image_array = np.array(image_data)\n",
    "        image_tensor = torch.from_numpy(image_array)\n",
    "        image_tensor = image_tensor.float()\n",
    "\n",
    "        label = self.labels_df.iloc[idx, 0]\n",
    "\n",
    "        return image_tensor, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dir = \"/home/nadjaflechner/Palsa_data/dataset_100m/\"\n",
    "# labels_file = \"/home/nadjaflechner/Palsa_data/binary_palsa_labels_100m.csv\"\n",
    "\n",
    "image_dir = \"/Users/nadja/Documents/UU/Thesis/Data/100m\"\n",
    "labels_file = \"/Users/nadja/Documents/UU/Thesis/Data/100m_palsa_labels.csv\"\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "labels_df = pd.read_csv(labels_file, index_col=0).head(100)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df = labels_df.head(800)\n",
    "val_df = labels_df.drop(train_df.index)\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "train_dataset = ImageDataset(image_dir, train_df )\n",
    "val_dataset = ImageDataset(image_dir, val_df )\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=20, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading VGG model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_BN_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_BN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/vgg16_bn-6c64b313.pth\" to /Users/nadja/.cache/torch/hub/checkpoints/vgg16_bn-6c64b313.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "VGG = models.vgg16_bn(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG.classifier = nn.Sequential(\n",
    "    nn.Linear(25088, 4096, bias = True),  \n",
    "    nn.ReLU(inplace=True),         \n",
    "    nn.Dropout(0.5, inplace=False),               \n",
    "    nn.Linear(in_features=4096, out_features=4096, bias=True),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5, inplace=False),\n",
    "    nn.Linear(in_features=4096, out_features=1000, bias=True),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5, inplace=False),\n",
    "    nn.Linear(in_features=1000, out_features=2, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/ finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# VGG = models.vgg16_bn(pretrained = True)\n",
    "VGG.eval()\n",
    "weights = models.VGG16_BN_Weights.DEFAULT\n",
    "transforms = weights.transforms()\n",
    "\n",
    "for imgs, labels in train_loader:\n",
    "    first_batch = imgs\n",
    "    first_labels = labels\n",
    "    break\n",
    "\n",
    "transformed_batch = transforms(first_batch)\n",
    "\n",
    "prediction = VGG(transformed_batch).softmax(1)\n",
    "class_scores, class_indices = torch.max(prediction, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5184, 0.5074, 0.5044, 0.5032, 0.5392, 0.5205, 0.5194, 0.5009, 0.5253,\n",
       "        0.5352, 0.5082, 0.5017, 0.5448, 0.5029, 0.5366, 0.5147, 0.5222, 0.5004,\n",
       "        0.5128, 0.5084], grad_fn=<MaxBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_only_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
