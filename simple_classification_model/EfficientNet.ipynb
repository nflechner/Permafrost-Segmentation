{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torchvision/image.so, 0x0006): Symbol not found: __ZN3c1017RegisterOperatorsD1Ev\n",
      "  Referenced from: <E03EDA44-89AE-3115-9796-62BA9E0E2EDE> /Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torchvision/image.so\n",
      "  Expected in:     <F75BB06B-2723-344D-99CE-9CB8BB94077A> /Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from torchvision import models, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = labels_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.index[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{img_name}.tif\")\n",
    "\n",
    "        # Open the TIF image using rasterio\n",
    "        with rasterio.open(img_path) as src:\n",
    "            # Read the image data\n",
    "            image_data = src.read()\n",
    "        image_array = np.array(image_data)\n",
    "        image_tensor = torch.from_numpy(image_array)\n",
    "        image_tensor = image_tensor.float()\n",
    "\n",
    "        label = self.labels_df.iloc[idx, 0]\n",
    "\n",
    "        return image_tensor, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dir = \"/home/nadjaflechner/Palsa_data/dataset_100m/\"\n",
    "# labels_file = \"/home/nadjaflechner/Palsa_data/binary_palsa_labels_100m.csv\"\n",
    "\n",
    "image_dir = \"/Users/nadja/Documents/UU/Thesis/Data/100m\"\n",
    "labels_file = \"/Users/nadja/Documents/UU/Thesis/Data/100m_palsa_labels.csv\"\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "labels_df = pd.read_csv(labels_file, index_col=0).head(100)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df = labels_df.head(80)\n",
    "val_df = labels_df.drop(train_df.index)\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "train_dataset = ImageDataset(image_dir, train_df )\n",
    "val_dataset = ImageDataset(image_dir, val_df )\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=5, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=5, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading EfficientNet model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B6_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B6_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "EfficientNet = models.efficientnet_b6(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EfficientNet.classifier = nn.Sequential(\n",
    "    nn.Linear(2304, 500, bias = True),  \n",
    "    nn.ReLU(inplace=True),         \n",
    "    nn.Dropout(0.5, inplace=False),               \n",
    "    nn.Linear(in_features=500, out_features=500, bias=True),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5, inplace=False),\n",
    "    nn.Linear(in_features=500, out_features=200, bias=True),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(p=0.5, inplace=False),\n",
    "    nn.Linear(in_features=200, out_features=2, bias=True)\n",
    ")\n",
    "\n",
    "# EfficientNet # to visually inspect model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/ finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# freeze all convolutional layers\n",
    "for param in EfficientNet.parameters():\n",
    "    if isinstance(param, nn.Conv2d):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# define model training parameters\n",
    "model = EfficientNet\n",
    "optimizer = optim.Adam(EfficientNet.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "num_epochs = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# define weights information \n",
    "weights = models.EfficientNet_B6_Weights.IMAGENET1K_V1\n",
    "transforms = weights.transforms()\n",
    "\n",
    "# Finetuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        transformed_batch = transforms(data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(transformed_batch)\n",
    "        loss = loss_function(outputs, targets.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"batch {batch_idx}/16 has been trained.\")\n",
    "\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "weights = models.EfficientNet_B6_Weights.IMAGENET1K_V1\n",
    "transforms = weights.transforms()\n",
    "\n",
    "for batch_idx, (data, targets) in enumerate(val_loader):\n",
    "    data, targets = data.to(device), targets.to(device)\n",
    "    transformed_batch = transforms(data)\n",
    "\n",
    "    prediction = model(transformed_batch).softmax(1)\n",
    "    class_scores, class_indices = torch.max(prediction, dim=1)\n",
    "\n",
    "    # Something with accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class activation maps? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nadja/miniconda3/envs/torchvision/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EfficientNet.eval()\n",
    "weights = models.EfficientNet_B6_Weights.IMAGENET1K_V1\n",
    "transforms = weights.transforms()\n",
    "\n",
    "for imgs, labels in train_loader:\n",
    "    first_batch = imgs\n",
    "    first_labels = labels\n",
    "    break\n",
    "\n",
    "transformed_batch = transforms(first_batch)\n",
    "\n",
    "prediction = EfficientNet(transformed_batch).softmax(1)\n",
    "class_scores, class_indices = torch.max(prediction, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9858, 0.5504], grad_fn=<MaxBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_only_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
