{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in labels to split test/train set\n",
    "\n",
    "palsa_labels = pd.read_csv(\"/home/nadjaflechner/Palsa_data/dataset_200m/palsa_labels.csv\", index_col= 0)\n",
    "palsa_labels.loc[palsa_labels['palsa_percentage'] > 0, 'palsa'] = 1\n",
    "palsa_labels.loc[palsa_labels['palsa_percentage'] == 0, 'palsa'] = 0\n",
    "\n",
    "binary_df = palsa_labels.drop('palsa_percentage', axis = 1)\n",
    "binary_df.to_csv(\"/home/nadjaflechner/Palsa_data/binary_palsa_labels_200m.csv\")\n",
    "\n",
    "\n",
    "# train = palsa_labels.sample(frac=0.8,random_state=200)\n",
    "# test = palsa_labels.drop(train.index)\n",
    "\n",
    "# # train = palsa_labels[:11000]\n",
    "# # test = palsa_labels[11000:14000]\n",
    "\n",
    "# train.to_csv(\"/home/nadjaflechner/Palsa_data/train_100mtest.csv\")\n",
    "# test.to_csv(\"/home/nadjaflechner/Palsa_data/test100mtesst.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67876, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palsa_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54300.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "67876*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palsa_percentage</th>\n",
       "      <th>palsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>758_66_55_2018_crop_8311</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758_66_55_2018_crop_8315</th>\n",
       "      <td>36</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758_66_55_2018_crop_8316</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758_66_55_2018_crop_8318</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          palsa_percentage  palsa\n",
       "758_66_55_2018_crop_8311                 8    1.0\n",
       "758_66_55_2018_crop_8315                36    1.0\n",
       "758_66_55_2018_crop_8316                 9    1.0\n",
       "758_66_55_2018_crop_8318                 0    0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palsa_labels.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "binary_df = palsa_labels.drop('palsa_percentage', axis = 1)\n",
    "binary_df.to_csv(\"/Users/nadja/Documents/UU/Thesis/generated_tifs/binary_palsa_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6459\n",
      "Epoch [2/10], Loss: 0.6217\n",
      "Epoch [3/10], Loss: 0.6687\n",
      "Epoch [4/10], Loss: 0.6557\n",
      "Epoch [5/10], Loss: 0.5872\n",
      "Epoch [6/10], Loss: 0.5863\n",
      "Epoch [7/10], Loss: 0.6598\n",
      "Epoch [8/10], Loss: 0.6956\n",
      "Epoch [9/10], Loss: 0.5488\n",
      "Epoch [10/10], Loss: 0.6538\n",
      "Validation Accuracy: 56.50%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = labels_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.index[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{img_name}.tif\")\n",
    "\n",
    "        # Open the TIF image using rasterio\n",
    "        with rasterio.open(img_path) as src:\n",
    "            # Read the image data\n",
    "            image_data = src.read()\n",
    "        image_array = np.array(image_data)\n",
    "        image_tensor = torch.from_numpy(image_array)\n",
    "        image_tensor = image_tensor.float()\n",
    "\n",
    "        label = self.labels_df.iloc[idx, 0]\n",
    "\n",
    "        return image_tensor, label\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(40000, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Set the paths and parameters\n",
    "image_dir = \"/home/nadjaflechner/Palsa_data/dataset_100m/\"\n",
    "labels_file = \"/home/nadjaflechner/Palsa_data/binary_palsa_labels_100m.csv\"\n",
    "batch_size = 40\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "labels_df = pd.read_csv(labels_file, index_col=0).head(3000)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df = labels_df.head(2400)\n",
    "val_df = labels_df.drop(train_df.index)\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "train_dataset = ImageDataset(image_dir, train_df)\n",
    "val_dataset = ImageDataset(image_dir, val_df)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create the CNN model\n",
    "model = CNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_dataloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"cnn_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below is current working version 24 april"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.5751\n",
      "Epoch [2/10], Loss: 0.6789\n",
      "Epoch [3/10], Loss: 0.6126\n",
      "Epoch [4/10], Loss: 0.6290\n",
      "Epoch [5/10], Loss: 0.6854\n",
      "Epoch [6/10], Loss: 0.6870\n",
      "Epoch [7/10], Loss: 0.6816\n",
      "Epoch [8/10], Loss: 0.6943\n",
      "Epoch [9/10], Loss: 0.6051\n",
      "Epoch [10/10], Loss: 0.6276\n",
      "Validation Accuracy: 60.23%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = labels_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.index[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{img_name}.tif\")\n",
    "\n",
    "        # Open the TIF image using rasterio\n",
    "        with rasterio.open(img_path) as src:\n",
    "            # Read the image data\n",
    "            image_data = src.read()\n",
    "        image_array = np.array(image_data)\n",
    "        image_tensor = torch.from_numpy(image_array)\n",
    "        image_tensor = image_tensor.float()\n",
    "\n",
    "        label = self.labels_df.iloc[idx, 0]\n",
    "\n",
    "        return image_tensor, label\n",
    "\n",
    "class PermafrostCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PermafrostCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=2, dilation=2)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=2, dilation=2)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.pool4 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.fc = nn.Linear(256, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "# Set the paths and parameters\n",
    "image_dir = \"/home/nadjaflechner/Palsa_data/dataset_100m/\"\n",
    "labels_file = \"/home/nadjaflechner/Palsa_data/binary_palsa_labels_100m.csv\"\n",
    "batch_size = 40\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "labels_df = pd.read_csv(labels_file, index_col=0).head(5000)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df = labels_df.head(2400)\n",
    "val_df = labels_df.drop(train_df.index)\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "train_dataset = ImageDataset(image_dir, train_df)\n",
    "val_dataset = ImageDataset(image_dir, val_df)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create the CNN model\n",
    "model = PermafrostCNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_dataloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"cnn_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 200x200 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = labels_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.index[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{img_name}.tif\")\n",
    "\n",
    "        # Open the TIF image using rasterio\n",
    "        with rasterio.open(img_path) as src:\n",
    "            # Read the image data\n",
    "            image_data = src.read()\n",
    "        image_array = np.array(image_data)\n",
    "        image_tensor = torch.from_numpy(image_array)\n",
    "        image_tensor = image_tensor.float()\n",
    "\n",
    "        label = self.labels_df.iloc[idx, 0]\n",
    "\n",
    "        return image_tensor, label\n",
    "\n",
    "class PermafrostCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PermafrostCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu5 = nn.ReLU(inplace=True)\n",
    "        self.pool5 = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.fc = nn.Linear(512, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.pool4(x)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.relu5(x)\n",
    "        x = self.pool5(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "# Set the paths and parameters\n",
    "image_dir = \"/home/nadjaflechner/Palsa_data/dataset_200m/\"\n",
    "labels_file = \"/home/nadjaflechner/Palsa_data/binary_palsa_labels_200m.csv\"\n",
    "batch_size = 40\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "labels_df = pd.read_csv(labels_file, index_col=0).head(5000)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df = labels_df.head(2400)\n",
    "val_df = labels_df.drop(train_df.index)\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "train_dataset = ImageDataset(image_dir, train_df)\n",
    "val_dataset = ImageDataset(image_dir, val_df)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create the CNN model\n",
    "model = PermafrostCNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_dataloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"cnn_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palsa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
