{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in labels to split test/train set\n",
    "\n",
    "palsa_labels = pd.read_csv(\"/home/nadjaflechner/Palsa_data/dataset_100m/palsa_labels.csv\", index_col= 0)\n",
    "palsa_labels.loc[palsa_labels['palsa_percentage'] > 0, 'palsa'] = 1\n",
    "palsa_labels.loc[palsa_labels['palsa_percentage'] == 0, 'palsa'] = 0\n",
    "\n",
    "binary_df = palsa_labels.drop('palsa_percentage', axis = 1)\n",
    "binary_df.to_csv(\"/home/nadjaflechner/Palsa_data/binary_palsa_labels_100m.csv\")\n",
    "\n",
    "\n",
    "# train = palsa_labels.sample(frac=0.8,random_state=200)\n",
    "# test = palsa_labels.drop(train.index)\n",
    "\n",
    "# # train = palsa_labels[:11000]\n",
    "# # test = palsa_labels[11000:14000]\n",
    "\n",
    "# train.to_csv(\"/home/nadjaflechner/Palsa_data/train_100mtest.csv\")\n",
    "# test.to_csv(\"/home/nadjaflechner/Palsa_data/test100mtesst.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67876, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palsa_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54300.8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "67876*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palsa_percentage</th>\n",
       "      <th>palsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>758_66_55_2018_crop_8311</th>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758_66_55_2018_crop_8315</th>\n",
       "      <td>36</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758_66_55_2018_crop_8316</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>758_66_55_2018_crop_8318</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          palsa_percentage  palsa\n",
       "758_66_55_2018_crop_8311                 8    1.0\n",
       "758_66_55_2018_crop_8315                36    1.0\n",
       "758_66_55_2018_crop_8316                 9    1.0\n",
       "758_66_55_2018_crop_8318                 0    0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palsa_labels.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "binary_df = palsa_labels.drop('palsa_percentage', axis = 1)\n",
    "binary_df.to_csv(\"/Users/nadja/Documents/UU/Thesis/generated_tifs/binary_palsa_labels.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 23 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 87\u001b[0m\n\u001b[1;32m     85\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     86\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m---> 87\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     88\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     89\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/palsa_env_torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/palsa_env_torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/palsa_env_torch/lib/python3.12/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniconda3/envs/palsa_env_torch/lib/python3.12/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 23 is out of bounds."
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## WHEN RUNNING MUST BE IN TORCH ENV ##\n",
    "########################################\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = labels_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.index[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{img_name}.tif\")\n",
    "\n",
    "        # Open the TIF image using rasterio\n",
    "        with rasterio.open(img_path) as src:\n",
    "            # Read the image data\n",
    "            image_data = src.read()\n",
    "        image_array = np.array(image_data)\n",
    "        image_tensor = torch.from_numpy(image_array)\n",
    "        image_tensor = image_tensor.float()\n",
    "\n",
    "        label = self.labels_df.iloc[idx, 0]\n",
    "\n",
    "        return image_tensor, label\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(32 * 50 * 50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Set the paths and parameters\n",
    "image_dir = \"/home/nadjaflechner/Palsa_data/dataset_100m/\"\n",
    "labels_file = \"/home/nadjaflechner/Palsa_data/train_100mtest.csv\"\n",
    "batch_size = 40\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "labels_df = pd.read_csv(labels_file, index_col=0)\n",
    "\n",
    "# Create the dataset and data loader\n",
    "dataset = ImageDataset(image_dir, labels_df)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Create the CNN model\n",
    "model = CNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"cnn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 20.4804\n",
      "Epoch [2/10], Loss: 5.1656\n",
      "Epoch [3/10], Loss: 1.1930\n",
      "Epoch [4/10], Loss: 0.6793\n",
      "Epoch [5/10], Loss: 0.2312\n",
      "Epoch [6/10], Loss: 0.4751\n",
      "Epoch [7/10], Loss: 1.3029\n",
      "Epoch [8/10], Loss: 0.4788\n",
      "Epoch [9/10], Loss: 0.3540\n",
      "Epoch [10/10], Loss: 0.1911\n",
      "Validation Accuracy: 82.35%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = labels_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.index[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{img_name}.tif\")\n",
    "\n",
    "        # Open the TIF image using rasterio\n",
    "        with rasterio.open(img_path) as src:\n",
    "            # Read the image data\n",
    "            image_data = src.read()\n",
    "        image_array = np.array(image_data)\n",
    "        image_tensor = torch.from_numpy(image_array)\n",
    "        image_tensor = image_tensor.float()\n",
    "\n",
    "        label = self.labels_df.iloc[idx, 0]\n",
    "\n",
    "        return image_tensor, label\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(32 * 50 * 50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # print(x.shape)\n",
    "        # x = self.conv3(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.maxpool(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Set the paths and parameters\n",
    "image_dir = \"/home/nadjaflechner/Palsa_data/dataset_100m/\"\n",
    "labels_file = \"/home/nadjaflechner/Palsa_data/binary_palsa_labels_100m.csv\"\n",
    "batch_size = 40\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "labels_df = pd.read_csv(labels_file, index_col=0)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df = labels_df.sample(frac=0.8,random_state=200)\n",
    "val_df = labels_df.drop(train_df.index)\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "train_dataset = ImageDataset(image_dir, train_df)\n",
    "val_dataset = ImageDataset(image_dir, val_df)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create the CNN model\n",
    "model = CNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_dataloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"cnn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 60.0000\n",
      "Epoch [2/10], Loss: 40.0000\n",
      "Epoch [3/10], Loss: 40.0000\n",
      "Epoch [4/10], Loss: 40.0000\n",
      "Epoch [5/10], Loss: 40.0000\n",
      "Epoch [6/10], Loss: 80.0000\n",
      "Epoch [7/10], Loss: 40.0000\n",
      "Epoch [8/10], Loss: 20.0000\n",
      "Epoch [9/10], Loss: 20.0000\n",
      "Epoch [10/10], Loss: 60.0000\n",
      "Validation Accuracy: 54.90%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = labels_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.index[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{img_name}.tif\")\n",
    "\n",
    "        # Open the TIF image using rasterio\n",
    "        with rasterio.open(img_path) as src:\n",
    "            # Read the image data\n",
    "            image_data = src.read()\n",
    "        image_array = np.array(image_data)\n",
    "        image_tensor = torch.from_numpy(image_array)\n",
    "        image_tensor = image_tensor.float()\n",
    "\n",
    "        label = self.labels_df.iloc[idx, 0]\n",
    "\n",
    "        return image_tensor, label\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 25 * 25, 500)\n",
    "        self.fc2 = nn.Linear(500, 1)\n",
    "        self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        # print(x.shape)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sig(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Set the paths and parameters\n",
    "image_dir = \"/Users/nadja/Documents/UU/Thesis/generated_tifs/\"\n",
    "labels_file = \"/Users/nadja/Documents/UU/Thesis/generated_tifs/binary_palsa_labels.csv\"\n",
    "batch_size = 40\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "labels_df = pd.read_csv(labels_file, index_col=0)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df = labels_df.sample(frac=0.8,random_state=200)\n",
    "val_df = labels_df.drop(train_df.index)\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "train_dataset = ImageDataset(image_dir, train_df)\n",
    "val_dataset = ImageDataset(image_dir, val_df)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create the CNN model\n",
    "model = CNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for images, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.float().unsqueeze(dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_dataloader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"cnn_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
      "        0., 0., 0., 0.], dtype=torch.float64) <class 'torch.Tensor'>\n",
      "tensor([1., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0.,\n",
      "        1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0.,\n",
      "        1., 1., 0., 1.], dtype=torch.float64) <class 'torch.Tensor'>\n",
      "tensor([0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1.], dtype=torch.float64) <class 'torch.Tensor'>\n",
      "tensor([1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1.,\n",
      "        1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 1.], dtype=torch.float64) <class 'torch.Tensor'>\n",
      "tensor([0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0.], dtype=torch.float64) <class 'torch.Tensor'>\n",
      "tensor([1., 0., 1., 1., 0.], dtype=torch.float64) <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_dataloader:\n",
    "    print(labels, type(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### MY EDIT OF THE ABOVE (EXPERIMENTATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 37 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 80\u001b[0m\n\u001b[1;32m     78\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     79\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m---> 80\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39mlong())\n\u001b[1;32m     81\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     82\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/palsa_env_torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/palsa_env_torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/palsa_env_torch/lib/python3.12/site-packages/torch/nn/modules/loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/miniconda3/envs/palsa_env_torch/lib/python3.12/site-packages/torch/nn/functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mIndexError\u001b[0m: Target 37 is out of bounds."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "\n",
    "# Define the custom dataset\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_df):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels_df = labels_df\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.index[idx]\n",
    "        img_path = os.path.join(self.image_dir, f\"{img_name}.tif\")\n",
    "\n",
    "        # Open the TIF image using rasterio\n",
    "        with rasterio.open(img_path) as src:\n",
    "            # Read the image data\n",
    "            image_data = src.read()\n",
    "        image_array = np.array(image_data)\n",
    "        image_tensor = torch.from_numpy(image_array)\n",
    "        image_tensor = image_tensor.float()\n",
    "\n",
    "        label = self.labels_df.iloc[idx, 0]\n",
    "\n",
    "        return image_tensor, label\n",
    "\n",
    "# Define the CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc = nn.Linear(32 * 50 * 50, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Set the paths and parameters\n",
    "image_dir = \"/Users/nadja/Documents/UU/Thesis/generated_tifs/\"\n",
    "labels_file = \"/Users/nadja/Documents/UU/Thesis/generated_tifs/binary_palsa_labels.csv\"\n",
    "batch_size = 40\n",
    "num_epochs = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Create the dataset and data loader\n",
    "dataset = ImageDataset(image_dir, train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Create the CNN model\n",
    "model = CNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"cnn_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "palsa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
