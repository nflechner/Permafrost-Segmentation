{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from utils import ImageDataset\n",
    "from VGG_model import vgg19\n",
    "\n",
    "\n",
    "#####################\n",
    "# Define dataloaders\n",
    "#####################\n",
    "\n",
    "\n",
    "# image_dir = \"/home/nadjaflechner/Palsa_data/dataset_100m/\"\n",
    "# labels_file = \"/home/nadjaflechner/Palsa_data/binary_palsa_labels_100m.csv\"\n",
    "\n",
    "image_dir = \"/Users/nadja/Documents/UU/Thesis/Data/100m\"\n",
    "labels_file = \"/Users/nadja/Documents/UU/Thesis/Data/100m_palsa_labels.csv\"\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "labels_df = pd.read_csv(labels_file, index_col=0).head(100)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df = labels_df.head(80)\n",
    "val_df = labels_df.drop(train_df.index)\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "train_dataset = ImageDataset(image_dir, train_df )\n",
    "val_dataset = ImageDataset(image_dir, val_df )\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ignore/vgg19-dcbb9e9d.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m###############\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Define model\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m###############\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m vgg19()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#freeze layers\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/Documents/UU/Thesis/palsa_seg/VGG_CAM/VGG_model.py:70\u001b[0m, in \u001b[0;36mvgg19\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvgg19\u001b[39m():\n\u001b[1;32m     69\u001b[0m     model \u001b[38;5;241m=\u001b[39m VGG(make_layers())\n\u001b[0;32m---> 70\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore/vgg19-dcbb9e9d.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     71\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/miniconda3/envs/torchvision/lib/python3.11/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/torchvision/lib/python3.11/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/torchvision/lib/python3.11/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ignore/vgg19-dcbb9e9d.pth'"
     ]
    }
   ],
   "source": [
    "###############\n",
    "# Define model\n",
    "###############\n",
    "\n",
    "model = vgg19()\n",
    "\n",
    "#freeze layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "#modify the last two convolutions\n",
    "model.features[-5] = nn.Conv2d(512,512,3, padding=1)\n",
    "model.features[-3] = nn.Conv2d(512,2,3, padding=1)\n",
    "\n",
    "#remove fully connected layer and replace it with AdaptiveAvePooling\n",
    "model.classifier = nn.Sequential(\n",
    "                                nn.AdaptiveAvgPool2d(1),\n",
    "                                nn.Flatten(),\n",
    "                                nn.LogSoftmax()\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Define training loop\n",
    "######################\n",
    "\n",
    "weights = models.VGG19_Weights.DEFAULT\n",
    "transforms = weights.transforms()\n",
    "\n",
    "# DON'T FORGET TO TRANSFORM BATCH!\n",
    "\n",
    "for imgs, labels in train_loader:\n",
    "    first_batch = imgs\n",
    "    first_labels = labels\n",
    "    break\n",
    "\n",
    "transformed_batch = transforms(first_batch)\n",
    "\n",
    "prediction = VGG(transformed_batch).softmax(1)\n",
    "class_scores, class_indices = torch.max(prediction, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COPIED FROM EFFICIENT NET, ADJUST FOR VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze all convolutional layers\n",
    "for param in EfficientNet.parameters():\n",
    "    if isinstance(param, nn.Conv2d):\n",
    "        param.requires_grad = False\n",
    "\n",
    "# define model training parameters\n",
    "model = EfficientNet\n",
    "optimizer = optim.Adam(EfficientNet.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "num_epochs = 1\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# define weights information \n",
    "weights = models.EfficientNet_B6_Weights.IMAGENET1K_V1\n",
    "transforms = weights.transforms()\n",
    "\n",
    "# Finetuning loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data, targets = data.to(device), targets.to(device)\n",
    "        transformed_batch = transforms(data)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(transformed_batch)\n",
    "        loss = loss_function(outputs, targets.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"batch {batch_idx}/16 has been trained.\")\n",
    "\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "weights = models.EfficientNet_B6_Weights.IMAGENET1K_V1\n",
    "transforms = weights.transforms()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "for batch_idx, (data, targets) in enumerate(val_loader):\n",
    "    data, targets = data.to(device), targets.to(device)\n",
    "    transformed_batch = transforms(data)\n",
    "\n",
    "    prediction = model(transformed_batch).softmax(1)\n",
    "    class_scores, class_indices = torch.max(prediction, dim=1)\n",
    "\n",
    "    total += targets.size(0)\n",
    "    correct += (class_indices == targets).sum().item()\n",
    "\n",
    "    break \n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Validation Accuracy: {accuracy:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchvision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
