{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Imports #\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from utils import  SaveFeatures, accuracy, imshow_transform\n",
    "from custom_model import model_4D\n",
    "from torch.autograd import Variable\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imshow\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, hs_dir, RGB_dir, labels_df, im_size):\n",
    "        self.RGB_dir = RGB_dir\n",
    "        self.hs_dir = hs_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.im_size = im_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.index[idx]\n",
    "        RGB_img_path = os.path.join(self.RGB_dir, f\"{img_name}.tif\")\n",
    "        hs_img_path = os.path.join(self.hs_dir, f\"{img_name}_hs.tif\")\n",
    "\n",
    "        with rasterio.open(RGB_img_path) as RGB_src:\n",
    "            # Read the image data\n",
    "            RGB_img = RGB_src.read()\n",
    "\n",
    "        with rasterio.open(hs_img_path) as hs_src:\n",
    "            # Read the image data\n",
    "            hs_img = hs_src.read()\n",
    "\n",
    "        # convert and upsample hs image\n",
    "        hs_image_array = np.array(hs_img)\n",
    "        hs_image_tensor = torch.from_numpy(hs_image_array)\n",
    "        hs_image_tensor = hs_image_tensor.float()\n",
    "        bilinear = nn.Upsample(size=self.im_size*2, mode='bilinear')\n",
    "        hs_upsampled_tensor = bilinear(hs_image_tensor.unsqueeze(0)).squeeze(0) \n",
    "\n",
    "        # converting RGB to tensor\n",
    "        RGB_image_array = np.array(RGB_img)\n",
    "        RGB_image_tensor = torch.from_numpy(RGB_image_array)\n",
    "        RGB_image_tensor = RGB_image_tensor.float()\n",
    "\n",
    "        combined_tensor = torch.concatenate((RGB_image_tensor, hs_upsampled_tensor))\n",
    "\n",
    "        means = combined_tensor.mean(dim=(1, 2), keepdim=True)\n",
    "        stds = combined_tensor.std(dim=(1, 2), keepdim=True)\n",
    "\n",
    "        # Normalize each map\n",
    "        normalized_tensor = (combined_tensor - means) / stds\n",
    "\n",
    "        label = self.labels_df.iloc[idx, 0]\n",
    "        label = 1 if label > 0 else 0\n",
    "\n",
    "        return normalized_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnadja-flechner\u001b[0m (\u001b[33mnadjaflechner\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nadjaflechner/palsa_seg/4dims_CNN/wandb/run-20240522_140359-i3b07wbp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nadjaflechner/VGG_CAMs/runs/i3b07wbp/workspace' target=\"_blank\">trim-fire-77</a></strong> to <a href='https://wandb.ai/nadjaflechner/VGG_CAMs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nadjaflechner/VGG_CAMs' target=\"_blank\">https://wandb.ai/nadjaflechner/VGG_CAMs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nadjaflechner/VGG_CAMs/runs/i3b07wbp/workspace' target=\"_blank\">https://wandb.ai/nadjaflechner/VGG_CAMs/runs/i3b07wbp/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nadjaflechner/miniconda3/envs/torch_only_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 136\u001b[0m\n\u001b[1;32m    133\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images) \n\u001b[1;32m    134\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, labels)\n\u001b[0;32m--> 136\u001b[0m val_acc\u001b[38;5;241m.\u001b[39mappend(\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    137\u001b[0m val_running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    138\u001b[0m val_batch_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/palsa_seg/4dims_CNN/utils.py:75\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(outputs, labels)\u001b[0m\n\u001b[1;32m     73\u001b[0m softmax \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSoftmax()\n\u001b[1;32m     74\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(softmax(outputs), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "###################\n",
    "# Hyperparameters #\n",
    "\n",
    "n_samples = 10000\n",
    "n_samples_train = int(round(n_samples*0.8))\n",
    "n_samples_val = int(round(n_samples*0.2))\n",
    "batch_size = 20\n",
    "current_computer =  \"ubuntu\" # \"macbook\" \n",
    "layers_to_freeze = 0\n",
    "lr = 0.00001\n",
    "weight_decay=0.04\n",
    "num_epochs = 10\n",
    "im_size = 200\n",
    "min_palsa_positive_samples = 10\n",
    "\n",
    "\n",
    "##########################\n",
    "# log hyperparams to w&b #\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"VGG_CAMs\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"n_samples\": n_samples,\n",
    "        \"layers_to_freeze\": layers_to_freeze,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"im_size\": im_size,\n",
    "        \"min_palsa_positive_samples\": min_palsa_positive_samples\n",
    "    },\n",
    "    tags=['4D', 'LRScheduler', 'MSELoss', 'TrainFromScratch', 'NormedImgs']\n",
    ")\n",
    "\n",
    "#############\n",
    "# Load data #\n",
    "\n",
    "if current_computer == \"ubuntu\":\n",
    "    hs_dir = f'/home/nadjaflechner/Palsa_data/cropped_hillshade_{im_size}m/hs'\n",
    "    RGB_dir = f'/home/nadjaflechner/Palsa_data/cropped_hillshade_{im_size}m/rgb'\n",
    "    labels_file = f'/home/nadjaflechner/Palsa_data/cropped_hillshade_{im_size}m/palsa_labels.csv'\n",
    "elif current_computer == \"macbook\":\n",
    "    hs_dir = '/Users/nadja/Documents/UU/Thesis/Data/cropped_hillshade/hs'\n",
    "    RGB_dir = '/Users/nadja/Documents/UU/Thesis/Data/cropped_hillshade/rgb'\n",
    "    labels_file = '/Users/nadja/Documents/UU/Thesis/Data/cropped_hillshade/palsa_labels.csv'\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "if min_palsa_positive_samples > 0:\n",
    "    labels_df = pd.read_csv(labels_file, index_col=0)\n",
    "    drop_range = labels_df[ (labels_df['palsa_percentage'] > 0) & (labels_df['palsa_percentage'] < min_palsa_positive_samples) ].index\n",
    "\n",
    "    labels_df.drop(drop_range, inplace=True)\n",
    "    labels_df = labels_df.head(n_samples)\n",
    "else: \n",
    "    labels_df = pd.read_csv(labels_file, index_col=0).head(n_samples)\n",
    "\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df = labels_df.head(n_samples_train)\n",
    "val_df = labels_df.drop(train_df.index)\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "train_dataset = ImageDataset(hs_dir, RGB_dir, train_df, im_size)\n",
    "val_dataset = ImageDataset(hs_dir, RGB_dir, val_df, im_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = model_4D()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.92)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "##################\n",
    "# Training loop #\n",
    "\n",
    "# adapted from https://github.com/tony-mtz/CAM/blob/master/network/net.py\n",
    "\n",
    "mean_train_losses = []\n",
    "mean_val_losses = []\n",
    "\n",
    "mean_train_acc = []\n",
    "mean_val_acc = []\n",
    "\n",
    "max_val_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('EPOCH: ',epoch+1)\n",
    "\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    train_batch_count = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):     \n",
    "\n",
    "        # load images and labels \n",
    "        images = Variable(images).to(device)  \n",
    "        labels = Variable(labels.long()).to(device)  \n",
    "\n",
    "        # train batch   \n",
    "        outputs = model(images) \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "\n",
    "        # calculate loss and accuracy\n",
    "        train_acc.append(accuracy(outputs, labels.long()))\n",
    "        running_loss += loss.item()\n",
    "        train_batch_count += 1\n",
    "\n",
    "    del images\n",
    "    del labels\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    model.eval()\n",
    "    val_batch_count = 0\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(val_loader):     \n",
    "            # load images and labels \n",
    "            images = Variable(images).to(device)  \n",
    "            labels = Variable(labels.long()).to(device)  \n",
    "            outputs = model(images) \n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            val_acc.append(accuracy(outputs, labels))\n",
    "            val_running_loss += loss.item()\n",
    "            val_batch_count +=1\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    del images\n",
    "    del labels\n",
    "    del outputs\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # update losses and accuracies \n",
    "\n",
    "    mean_train_acc.append(np.mean(train_acc))\n",
    "    mean_val_acc.append(np.mean(val_acc))\n",
    "    mean_train_losses.append(running_loss/train_batch_count)\n",
    "    mean_val_losses.append(val_running_loss/val_batch_count)\n",
    "\n",
    "    wandb.log({\"train_accuracy\": np.mean(train_acc)})\n",
    "    wandb.log({\"val_accuracy\": np.mean(val_acc)})\n",
    "    wandb.log({\"train_loss\": running_loss/train_batch_count})\n",
    "    wandb.log({\"val_loss\": val_running_loss/val_batch_count})\n",
    "\n",
    "    if np.mean(val_acc) > max_val_acc:\n",
    "        best_model = model.state_dict()\n",
    "        max_val_acc = np.mean(val_acc)\n",
    "\n",
    "\n",
    "torch.save(best_model, '/home/nadjaflechner/models/model.pth')\n",
    "artifact = wandb.Artifact('model', type='model')\n",
    "artifact.add_file('/home/nadjaflechner/models/model.pth')\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "##########################\n",
    "# plot loss and accuracy #\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,5))\n",
    "\n",
    "ax1.plot([i for i in range(num_epochs)], mean_val_losses, c = 'b', label = 'val loss')\n",
    "ax1.plot([i for i in range(num_epochs)], mean_train_losses, c = 'r', label = 'train loss')\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_xticks(range(0,num_epochs+1,5))\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot([i for i in range(num_epochs)], mean_val_acc, c = 'b', label = 'val accuracy')\n",
    "ax2.plot([i for i in range(num_epochs)], mean_train_acc, c = 'r', label = 'train accuracy')\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_xticks(range(0,num_epochs+1,5))\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "wandb.log({'model_performance': wandb.Image(fig)})\n",
    "\n",
    "####################\n",
    "# generating CAMs #\n",
    "\n",
    "#change batch size to 1 to grab one image at a time\n",
    "valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, \n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "\n",
    "# Load best model to produce CAMs with\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "# Save 10 palsa images\n",
    "palsa_imgs = 0\n",
    "for palsa_cam in range(100):\n",
    "    im, lab = next(iter(valid_loader))\n",
    "\n",
    "    #get the last convolution\n",
    "    sf = SaveFeatures(model.features[-4])\n",
    "    model.eval()\n",
    "\n",
    "    if lab.sum().detach().cpu().numpy() == 2: # if both classes present in image\n",
    "        palsa_imgs+= 1\n",
    "        im = Variable(im).to(device)\n",
    "        outputs = model(im).to(device)\n",
    "\n",
    "        # generate CAM\n",
    "        sf.remove()\n",
    "        arr = sf.features.cpu().detach().numpy()\n",
    "        arr1 = arr[0]\n",
    "        ans_nopalsa = np.dot(np.rollaxis(arr1,0,3), [1,0])\n",
    "        ans_palsa = np.dot(np.rollaxis(arr1,0,3), [0,1])\n",
    "\n",
    "        CAM_pals = resize(ans_palsa, (im_size*2,im_size*2))\n",
    "        CAM_nopals = resize(ans_nopalsa, (im_size*2,im_size*2))\n",
    "\n",
    "        # Plot image with CAM\n",
    "        cpu_img = im.squeeze().cpu().detach().permute(1,2,0).long().numpy()\n",
    "\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (15,6))\n",
    "\n",
    "        ax1.imshow(cpu_img)\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])\n",
    "        ax1.set_title(f'original image')\n",
    "\n",
    "        ax2.imshow(cpu_img)\n",
    "        ax2.imshow(CAM_pals, alpha=.4, cmap='jet')\n",
    "        ax2.set_xticks([])\n",
    "        ax2.set_yticks([])\n",
    "        ax2.set_title('Palsa feature map')\n",
    "\n",
    "        ax3.imshow(cpu_img)\n",
    "        ax3.imshow(CAM_nopals, alpha=.4, cmap='jet')\n",
    "        ax3.set_xticks([])\n",
    "        ax3.set_yticks([])\n",
    "        ax3.set_title('No_Palsa feature map')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        wandb.log({'generated_CAM': fig})\n",
    "\n",
    "    if palsa_imgs == 20:\n",
    "        break\n",
    "\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan],\n",
       "        [nan, nan]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger dataset from data augmentation! (but no normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Imports #\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from utils import  SaveFeatures, accuracy, imshow_transform\n",
    "from custom_model import model_4D\n",
    "from torch.autograd import Variable\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imshow\n",
    "import torchvision.transforms as transforms\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt \n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, hs_dir, RGB_dir, labels_df, im_size):\n",
    "        self.RGB_dir = RGB_dir\n",
    "        self.hs_dir = hs_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.im_size = im_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.index[idx]\n",
    "        RGB_img_path = os.path.join(self.RGB_dir, f\"{img_name}.tif\")\n",
    "        hs_img_path = os.path.join(self.hs_dir, f\"{img_name}_hs.tif\")\n",
    "\n",
    "        with rasterio.open(RGB_img_path) as RGB_src:\n",
    "            # Read the image data\n",
    "            RGB_img = RGB_src.read()\n",
    "\n",
    "        with rasterio.open(hs_img_path) as hs_src:\n",
    "            # Read the image data\n",
    "            hs_img = hs_src.read()\n",
    "\n",
    "        # convert and upsample hs image\n",
    "        hs_image_array = np.array(hs_img)\n",
    "        hs_image_tensor = torch.from_numpy(hs_image_array)\n",
    "        hs_image_tensor = hs_image_tensor.float()\n",
    "        bilinear = nn.Upsample(size=self.im_size*2, mode='bilinear')\n",
    "        hs_upsampled_tensor = bilinear(hs_image_tensor.unsqueeze(0)).squeeze(0) \n",
    "\n",
    "        # converting RGB to tensor\n",
    "        RGB_image_array = np.array(RGB_img)\n",
    "        RGB_image_tensor = torch.from_numpy(RGB_image_array)\n",
    "        RGB_image_tensor = RGB_image_tensor.float()\n",
    "\n",
    "        combined_tensor = torch.concatenate((RGB_image_tensor, hs_upsampled_tensor))\n",
    "\n",
    "        # means = combined_tensor.mean(dim=(1, 2), keepdim=True)\n",
    "        # stds = combined_tensor.std(dim=(1, 2), keepdim=True)\n",
    "\n",
    "        # # Normalize each map\n",
    "        # normalized_tensor = (combined_tensor - means) / stds\n",
    "\n",
    "        label = self.labels_df.iloc[idx, 0]\n",
    "        label = 1 if label > 0 else 0\n",
    "\n",
    "        return combined_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransImageDataset(Dataset):\n",
    "    def __init__(self, hs_dir, RGB_dir, labels_df, im_size):\n",
    "        self.RGB_dir = RGB_dir\n",
    "        self.hs_dir = hs_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.im_size = im_size\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.labels_df.index[idx]\n",
    "        RGB_img_path = os.path.join(self.RGB_dir, f\"{img_name}.tif\")\n",
    "        hs_img_path = os.path.join(self.hs_dir, f\"{img_name}_hs.tif\")\n",
    "\n",
    "        with rasterio.open(RGB_img_path) as RGB_src:\n",
    "            # Read the image data\n",
    "            RGB_img = RGB_src.read()\n",
    "\n",
    "        with rasterio.open(hs_img_path) as hs_src:\n",
    "            # Read the image data\n",
    "            hs_img = hs_src.read()\n",
    "\n",
    "        # convert and upsample hs image\n",
    "        hs_image_array = np.array(hs_img)\n",
    "        hs_image_tensor = torch.from_numpy(hs_image_array)\n",
    "        hs_image_tensor = hs_image_tensor.float()\n",
    "        bilinear = nn.Upsample(size=self.im_size*2, mode='bilinear')\n",
    "        hs_upsampled_tensor = bilinear(hs_image_tensor.unsqueeze(0)).squeeze(0) \n",
    "\n",
    "        # converting RGB to tensor\n",
    "        RGB_image_array = np.array(RGB_img)\n",
    "        RGB_image_tensor = torch.from_numpy(RGB_image_array)\n",
    "        RGB_image_tensor = RGB_image_tensor.float()\n",
    "\n",
    "        combined_tensor = torch.concatenate((RGB_image_tensor, hs_upsampled_tensor))\n",
    "        transformed_tensor = self.transforms(combined_tensor)\n",
    "\n",
    "        label = self.labels_df.iloc[idx, 0]\n",
    "        label = 1 if label > 0 else 0\n",
    "\n",
    "        return transformed_tensor, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnadja-flechner\u001b[0m (\u001b[33mnadjaflechner\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/nadjaflechner/palsa_seg/4dims_CNN/wandb/run-20240522_141247-rj28ze5j</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nadjaflechner/VGG_CAMs/runs/rj28ze5j/workspace' target=\"_blank\">atomic-morning-78</a></strong> to <a href='https://wandb.ai/nadjaflechner/VGG_CAMs' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nadjaflechner/VGG_CAMs' target=\"_blank\">https://wandb.ai/nadjaflechner/VGG_CAMs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nadjaflechner/VGG_CAMs/runs/rj28ze5j/workspace' target=\"_blank\">https://wandb.ai/nadjaflechner/VGG_CAMs/runs/rj28ze5j/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nadjaflechner/miniconda3/envs/torch_only_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  2\n",
      "EPOCH:  3\n",
      "EPOCH:  4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###################\n",
    "# Hyperparameters #\n",
    "\n",
    "n_samples = 10000\n",
    "n_samples_train = int(round(n_samples*0.8))\n",
    "n_samples_val = int(round(n_samples*0.2))\n",
    "batch_size = 20\n",
    "current_computer =  \"ubuntu\" # \"macbook\" \n",
    "layers_to_freeze = 0\n",
    "lr = 0.00001\n",
    "weight_decay=0.04\n",
    "num_epochs = 10\n",
    "im_size = 200\n",
    "min_palsa_positive_samples = 10\n",
    "\n",
    "\n",
    "##########################\n",
    "# log hyperparams to w&b #\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"VGG_CAMs\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"n_samples\": n_samples,\n",
    "        \"layers_to_freeze\": layers_to_freeze,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"im_size\": im_size,\n",
    "        \"min_palsa_positive_samples\": min_palsa_positive_samples\n",
    "    },\n",
    "    tags=['4D', 'LRScheduler', 'MSELoss', 'TrainFromScratch', 'NormedImgs']\n",
    ")\n",
    "\n",
    "#############\n",
    "# Load data #\n",
    "\n",
    "if current_computer == \"ubuntu\":\n",
    "    hs_dir = f'/home/nadjaflechner/Palsa_data/cropped_hillshade_{im_size}m/hs'\n",
    "    RGB_dir = f'/home/nadjaflechner/Palsa_data/cropped_hillshade_{im_size}m/rgb'\n",
    "    labels_file = f'/home/nadjaflechner/Palsa_data/cropped_hillshade_{im_size}m/palsa_labels.csv'\n",
    "elif current_computer == \"macbook\":\n",
    "    hs_dir = '/Users/nadja/Documents/UU/Thesis/Data/cropped_hillshade/hs'\n",
    "    RGB_dir = '/Users/nadja/Documents/UU/Thesis/Data/cropped_hillshade/rgb'\n",
    "    labels_file = '/Users/nadja/Documents/UU/Thesis/Data/cropped_hillshade/palsa_labels.csv'\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "if min_palsa_positive_samples > 0:\n",
    "    labels_df = pd.read_csv(labels_file, index_col=0)\n",
    "    drop_range = labels_df[ (labels_df['palsa_percentage'] > 0) & (labels_df['palsa_percentage'] < min_palsa_positive_samples) ].index\n",
    "\n",
    "    labels_df.drop(drop_range, inplace=True)\n",
    "    labels_df = labels_df.head(n_samples)\n",
    "else: \n",
    "    labels_df = pd.read_csv(labels_file, index_col=0).head(n_samples)\n",
    "\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df = labels_df.head(n_samples_train)\n",
    "val_df = labels_df.drop(train_df.index)\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "train_dataset = ImageDataset(hs_dir, RGB_dir, train_df, im_size)\n",
    "val_dataset = ImageDataset(hs_dir, RGB_dir, val_df, im_size)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "trans_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = model_4D()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.92)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "##################\n",
    "# Training loop #\n",
    "\n",
    "# adapted from https://github.com/tony-mtz/CAM/blob/master/network/net.py\n",
    "\n",
    "mean_train_losses = []\n",
    "mean_val_losses = []\n",
    "\n",
    "mean_train_acc = []\n",
    "mean_val_acc = []\n",
    "\n",
    "max_val_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('EPOCH: ',epoch+1)\n",
    "\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    train_batch_count = 0\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):     \n",
    "\n",
    "        # load images and labels \n",
    "        images = Variable(images).to(device)  \n",
    "        labels = Variable(labels.long()).to(device)  \n",
    "\n",
    "        # train batch   \n",
    "        outputs = model(images) \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "\n",
    "        # calculate loss and accuracy\n",
    "        train_acc.append(accuracy(outputs, labels.long()))\n",
    "        running_loss += loss.item()\n",
    "        train_batch_count += 1\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    NOTE:\n",
    "    The way in which training of transformed data is implemented here is probably \n",
    "    undesirable because it first trains a whole epoch on normal images and then \n",
    "    transformed. Ideally would probably be shuffled.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Train on transformed data \n",
    "    for batch_idx, (images, labels) in enumerate(trans_loader):     \n",
    "\n",
    "        # load images and labels \n",
    "        images = Variable(images).to(device)  \n",
    "        labels = Variable(labels.long()).to(device)  \n",
    "\n",
    "        # train batch   \n",
    "        outputs = model(images) \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "\n",
    "        # calculate loss and accuracy\n",
    "        train_acc.append(accuracy(outputs, labels.long()))\n",
    "        running_loss += loss.item()\n",
    "        train_batch_count += 1\n",
    "\n",
    "    model.eval()\n",
    "    val_batch_count = 0\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(val_loader):     \n",
    "            # load images and labels \n",
    "            images = Variable(images).to(device)  \n",
    "            labels = Variable(labels.long()).to(device)  \n",
    "            outputs = model(images) \n",
    "            loss = loss_function(outputs, labels)\n",
    "\n",
    "            val_acc.append(accuracy(outputs, labels))\n",
    "            val_running_loss += loss.item()\n",
    "            val_batch_count +=1\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # update losses and accuracies \n",
    "\n",
    "    mean_train_acc.append(np.mean(train_acc))\n",
    "    mean_val_acc.append(np.mean(val_acc))\n",
    "    mean_train_losses.append(running_loss/train_batch_count)\n",
    "    mean_val_losses.append(val_running_loss/val_batch_count)\n",
    "\n",
    "    wandb.log({\"train_accuracy\": np.mean(train_acc)})\n",
    "    wandb.log({\"val_accuracy\": np.mean(val_acc)})\n",
    "    wandb.log({\"train_loss\": running_loss/train_batch_count})\n",
    "    wandb.log({\"val_loss\": val_running_loss/val_batch_count})\n",
    "\n",
    "    if np.mean(val_acc) > max_val_acc:\n",
    "        best_model = model.state_dict()\n",
    "        max_val_acc = np.mean(val_acc)\n",
    "\n",
    "\n",
    "torch.save(best_model, '/home/nadjaflechner/models/model.pth')\n",
    "artifact = wandb.Artifact('model', type='model')\n",
    "artifact.add_file('/home/nadjaflechner/models/model.pth')\n",
    "run.log_artifact(artifact)\n",
    "\n",
    "##########################\n",
    "# plot loss and accuracy #\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,5))\n",
    "\n",
    "ax1.plot([i for i in range(num_epochs)], mean_val_losses, c = 'b', label = 'val loss')\n",
    "ax1.plot([i for i in range(num_epochs)], mean_train_losses, c = 'r', label = 'train loss')\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_xticks(range(0,num_epochs+1,5))\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot([i for i in range(num_epochs)], mean_val_acc, c = 'b', label = 'val accuracy')\n",
    "ax2.plot([i for i in range(num_epochs)], mean_train_acc, c = 'r', label = 'train accuracy')\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_xticks(range(0,num_epochs+1,5))\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "wandb.log({'model_performance': wandb.Image(fig)})\n",
    "\n",
    "####################\n",
    "# generating CAMs #\n",
    "\n",
    "#change batch size to 1 to grab one image at a time\n",
    "valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, \n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "\n",
    "# Load best model to produce CAMs with\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "# Save 10 palsa images\n",
    "palsa_imgs = 0\n",
    "for palsa_cam in range(100):\n",
    "    im, lab = next(iter(valid_loader))\n",
    "\n",
    "    #get the last convolution\n",
    "    sf = SaveFeatures(model.features[-4])\n",
    "    model.eval()\n",
    "\n",
    "    if lab.sum().detach().cpu().numpy() == 2: # if both classes present in image\n",
    "        palsa_imgs+= 1\n",
    "        im = Variable(im).to(device)\n",
    "        outputs = model(im).to(device)\n",
    "\n",
    "        # generate CAM\n",
    "        sf.remove()\n",
    "        arr = sf.features.cpu().detach().numpy()\n",
    "        arr1 = arr[0]\n",
    "        ans_nopalsa = np.dot(np.rollaxis(arr1,0,3), [1,0])\n",
    "        ans_palsa = np.dot(np.rollaxis(arr1,0,3), [0,1])\n",
    "\n",
    "        CAM_pals = resize(ans_palsa, (im_size*2,im_size*2))\n",
    "        CAM_nopals = resize(ans_nopalsa, (im_size*2,im_size*2))\n",
    "\n",
    "        # Plot image with CAM\n",
    "        cpu_img = im.squeeze().cpu().detach().permute(1,2,0).long().numpy()\n",
    "\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1,3, figsize = (15,6))\n",
    "\n",
    "        ax1.imshow(cpu_img)\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])\n",
    "        ax1.set_title(f'original image')\n",
    "\n",
    "        ax2.imshow(cpu_img)\n",
    "        ax2.imshow(CAM_pals, alpha=.4, cmap='jet')\n",
    "        ax2.set_xticks([])\n",
    "        ax2.set_yticks([])\n",
    "        ax2.set_title('Palsa feature map')\n",
    "\n",
    "        ax3.imshow(cpu_img)\n",
    "        ax3.imshow(CAM_nopals, alpha=.4, cmap='jet')\n",
    "        ax3.set_xticks([])\n",
    "        ax3.set_yticks([])\n",
    "        ax3.set_title('No_Palsa feature map')\n",
    "\n",
    "        plt.tight_layout()\n",
    "\n",
    "        wandb.log({'generated_CAM': fig})\n",
    "\n",
    "    if palsa_imgs == 20:\n",
    "        break\n",
    "\n",
    "\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_only_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
