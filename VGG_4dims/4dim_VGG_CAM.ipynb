{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "labels_df = pd.read_csv(\"/Users/nadja/Documents/UU/Thesis/Data/cropped_hillshade/palsa_labels.csv\", index_col=0).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Imports #\n",
    "\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from torchvision import models, transforms\n",
    "from utils import ImageDataset, SaveFeatures, accuracy, imshow_transform\n",
    "from VGG_16bn import vgg16bn\n",
    "from torch.autograd import Variable\n",
    "from skimage.transform import resize\n",
    "from skimage.io import imshow\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "###################\n",
    "# Hyperparameters #\n",
    "\n",
    "n_samples = 5\n",
    "n_samples_train = int(round(n_samples*0.8))\n",
    "batch_size = 1\n",
    "current_computer =  \"macbook\" # \"ubuntu\"\n",
    "layers_to_freeze = 41\n",
    "lr = 0.00001\n",
    "weight_decay=0.09\n",
    "num_epochs = 15\n",
    "im_size = 100\n",
    "min_palsa_positive_samples = 10\n",
    "\n",
    "\n",
    "##########################\n",
    "# log hyperparams to w&b #\n",
    "\n",
    "run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"VGG_CAMs\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": num_epochs,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"n_samples\": n_samples,\n",
    "        \"layers_to_freeze\": layers_to_freeze,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"im_size\": im_size,\n",
    "        \"min_palsa_positive_samples\": min_palsa_positive_samples\n",
    "    },\n",
    ")\n",
    "\n",
    "#############\n",
    "# Load data #\n",
    "\n",
    "if current_computer == \"ubuntu\":\n",
    "    image_dir = f\"/home/nadjaflechner/Palsa_data/dataset_{im_size}m/\"\n",
    "    labels_file = f\"/home/nadjaflechner/Palsa_data/binary_palsa_labels_{im_size}m.csv\"\n",
    "elif current_computer == \"macbook\":\n",
    "    image_dir = f\"/Users/nadja/Documents/UU/Thesis/Data/{im_size}m\"\n",
    "    labels_file = f\"/Users/nadja/Documents/UU/Thesis/Data/{im_size}m_palsa_labels.csv\"\n",
    "\n",
    "# Load the labels from the CSV file\n",
    "labels_df = pd.read_csv(labels_file, index_col=0).head(n_samples)\n",
    "if min_palsa_positive_samples > 0:\n",
    "    drop_range = labels_df[ (labels_df['palsa_percentage'] > 0) & (labels_df['palsa_percentage'] <= 10) ].index\n",
    "    labels_df.drop(drop_range, inplace=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_df = labels_df.head(n_samples_train)\n",
    "val_df = labels_df.drop(train_df.index)\n",
    "\n",
    "# Create the datasets and data loaders\n",
    "train_dataset = ImageDataset(image_dir, train_df)\n",
    "val_dataset = ImageDataset(image_dir, val_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Define model \n",
    "\n",
    "model = vgg16bn()\n",
    "\n",
    "#freeze layers\n",
    "for idx, param in enumerate(model.parameters()):\n",
    "    if idx == layers_to_freeze:\n",
    "        break\n",
    "    param.requires_grad = False\n",
    "\n",
    "#modify the last two convolutions\n",
    "model.features[-7] = nn.Conv2d(512,512,3, padding=1)\n",
    "model.features[-4] = nn.Conv2d(512,2,3, padding=1)\n",
    "model.features[-3] = nn.BatchNorm2d(2,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "\n",
    "#remove fully connected layer and replace it with AdaptiveAvePooling\n",
    "model.classifier = nn.Sequential(\n",
    "                                nn.AdaptiveAvgPool2d(1),\n",
    "                                nn.Flatten()\n",
    "                                )\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, param in enumerate(model.parameters()):\n",
    "    print(f'param {idx}: {param.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Training loop #\n",
    "\n",
    "# adapted from https://github.com/tony-mtz/CAM/blob/master/network/net.py\n",
    "\n",
    "weights = models.VGG16_BN_Weights.DEFAULT\n",
    "transforms = weights.transforms()\n",
    "\n",
    "mean_train_losses = []\n",
    "mean_val_losses = []\n",
    "\n",
    "mean_train_acc = []\n",
    "mean_val_acc = []\n",
    "\n",
    "max_val_acc = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('EPOCH: ',epoch+1)\n",
    "\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    train_batch_count = 0\n",
    "    for batch_idx, (images, labels) in enumerate(train_loader):     \n",
    "\n",
    "        # load images and labels \n",
    "        images = Variable(transforms(images)).to(device)\n",
    "        labels = Variable(labels.long()).to(device)\n",
    "\n",
    "        # train batch   \n",
    "        outputs = model(images)         \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()  \n",
    "\n",
    "        # calculate loss and accuracy\n",
    "        train_acc.append(accuracy(outputs, labels.long()))\n",
    "        running_loss += loss.item()\n",
    "        train_batch_count += 1\n",
    "    \n",
    "    model.eval()\n",
    "    val_batch_count = 0\n",
    "    val_running_loss = 0.0\n",
    "    model.to(device)\n",
    "\n",
    "    for batch_idx, (images, labels) in enumerate(val_loader):     \n",
    "        # inference   \n",
    "        images = Variable(transforms(images)).to(device)\n",
    "        labels = Variable(labels.long()).to(device)\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "\n",
    "        val_acc.append(accuracy(outputs, labels))\n",
    "        val_running_loss += loss.item()\n",
    "        val_batch_count +=1\n",
    "\n",
    "    # update losses and accuracies \n",
    "\n",
    "    mean_train_acc.append(np.mean(train_acc))\n",
    "    mean_val_acc.append(np.mean(val_acc))\n",
    "    mean_train_losses.append(running_loss/train_batch_count)\n",
    "    mean_val_losses.append(val_running_loss/val_batch_count)\n",
    "\n",
    "    wandb.log({\"train_accuracy\": np.mean(train_acc)})\n",
    "    wandb.log({\"val_accuracy\": np.mean(val_acc)})\n",
    "    wandb.log({\"train_loss\": running_loss/train_batch_count})\n",
    "    wandb.log({\"val_loss\": val_running_loss/val_batch_count})\n",
    "\n",
    "    if np.mean(val_acc) > max_val_acc:\n",
    "        best_model = model.state_dict()\n",
    "        max_val_acc = np.mean(val_acc)\n",
    "\n",
    "torch.save(best_model, '/home/nadjaflechner/models/model.pth')\n",
    "artifact = wandb.Artifact('model', type='model')\n",
    "artifact.add_file('/home/nadjaflechner/models/model.pth')\n",
    "run.log_artifact(artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# plot loss and accuracy #\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,5))\n",
    "\n",
    "ax1.plot([i for i in range(num_epochs)], mean_val_losses, c = 'b', label = 'val loss')\n",
    "ax1.plot([i for i in range(num_epochs)], mean_train_losses, c = 'r', label = 'train loss')\n",
    "ax1.set_xlabel(\"Epochs\")\n",
    "ax1.set_xticks(range(0,num_epochs+1,5))\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot([i for i in range(num_epochs)], mean_val_acc, c = 'b', label = 'val accuracy')\n",
    "ax2.plot([i for i in range(num_epochs)], mean_train_acc, c = 'r', label = 'train accuracy')\n",
    "ax2.set_xlabel(\"Epochs\")\n",
    "ax2.set_xticks(range(0,num_epochs+1,5))\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "wandb.log({'model_performance': wandb.Image(fig)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# generating CAMs #\n",
    "\n",
    "#change batch size to 1 to grab one image at a time\n",
    "valid_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, \n",
    "                                          shuffle=True,\n",
    "                                          num_workers=1)\n",
    "\n",
    "# Load best model to produce CAMs with\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "\n",
    "# Save 10 palsa images\n",
    "palsa_imgs = 0\n",
    "for palsa_cam in range(100):\n",
    "    im, lab = next(iter(valid_loader))\n",
    "\n",
    "    #get the last convolution\n",
    "    sf = SaveFeatures(model.features[-4])\n",
    "    model.eval()\n",
    "\n",
    "    if lab == 1:\n",
    "        palsa_imgs+= 1\n",
    "        im = Variable(im).to(device)\n",
    "        outputs = model(transforms(im)).to(device)\n",
    "        res = torch.argmax(outputs.data).cpu().detach().numpy()\n",
    "\n",
    "        # generate CAM\n",
    "        sf.remove()\n",
    "        arr = sf.features.cpu().detach().numpy()\n",
    "        arr1 = arr[0]\n",
    "        ans_nopalsa = np.dot(np.rollaxis(arr1,0,3), [1,0])\n",
    "        ans_palsa = np.dot(np.rollaxis(arr1,0,3), [0,1])\n",
    "\n",
    "        if res==1:\n",
    "            CAM = resize(ans_palsa, (im_size*2,im_size*2))\n",
    "        else:\n",
    "            CAM = resize(ans_nopalsa, (im_size*2,im_size*2))\n",
    "\n",
    "        # Plot image with CAM\n",
    "        cpu_img = im.squeeze().cpu().detach().permute(1,2,0).long().numpy()\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,7))\n",
    "\n",
    "        ax1.imshow(cpu_img)\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])\n",
    "        ax1.set_title('original image')\n",
    "\n",
    "        ax2.imshow(cpu_img)\n",
    "        ax2.imshow(CAM, alpha=.4, cmap='jet')\n",
    "        ax2.set_xticks([])\n",
    "        ax2.set_yticks([])\n",
    "        ax2.set_title('image with CAM')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "        wandb.log({'generated_CAM': fig})\n",
    "\n",
    "    if palsa_imgs == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOGGING FORGOTTEN CAMS FROM SAVED MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load model again: \n",
    "\n",
    "run = wandb.init(project= 'VGG_CAMs', id= 'v6ax9crk', resume = 'must')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "artifact = run.use_artifact('nadjaflechner/VGG_CAMs/model:v9', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "state_dict = torch.load(f\"{artifact_dir}/model.pth\")\n",
    "model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_only_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
